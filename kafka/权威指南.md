# kafka

标签（空格分隔）： kafka

---

> kafka的feature特性？用来堆栈存放请求的？

最简单的提交方式是让消费者自动提交偏移量。如果enable_auto_commit 被设为 true ，那么每过5秒，消费者会自动把从poll （） 方法接收到的最大偏移量提交上去。

Q: kafka是怎么自动提交的？每过5秒提交？Python怎么发起的？

A: kafka 在每次pull_once的时候会去**判断一下时间，如果每次时间适合了，会发起异步自动提交offset的操作**。可以通过搜索一下相关配置的关键词找到相关的处理


poll （） 方法能返回一个记录列表。每条记录都包含了记录所属主题的信息、记 在分区的信息 记录在分区里的偏移量 ，以及记录的键值对。我们 般会遍历这个列表 ，逐条处理这些记录。

poll （） 方法有一个超时参数它指定了方法在多久之后可以返回，不管**有没有可用数据都要返回**，可以看下kafka-python的poll源码。超时时间的设置取决于应用程序对响应速度的要求，比如要在多长内**把控制权归还给执行轮询的线程**。

也就是timeout_ms参数，跟fetch_max_wait_ms是有区别的。这个是控制kafka多久返回一次消息，用于指定broker的等待时间，默认是500ms。

consumer_timeout_ms：若不指定consumer_timeout_ms，message iteration迭代中，默认一直循环等待接收，若指定，则超时返回，不再等待。

fetch.min.bytes :这样可以**降低消费者和 broker的工作负载**，因为它们在主题不是很活跃的时候（或者一天里的低谷时段）就不需要来来回回地处理消息。如果没有很多可用数据，但消费者的 CPU使用率却很高，那么就需要把该属性的值设得比默认值大。如果消费者的数量比较多，把该属性的值设置得大一点可以**降低 broker 的工作负载**（避免过多的请求）。


---


消息被分批次写入 Kafka 批次就是一组消息，这些消息属于同一 主题和分区。减少网络开销

这要在时间延迟和吞吐量之间作出权衡：批次越大，单位时间内处理的消息就越多，**单个消息的传输时间就越长**。批次数据会被压缩，这样可以**提高数据的传输和存储能力**，但要做**更多的计算处理**。

**Avro** 提供了一种紧凑的序列化 格式，模式和消息体是分开的，当模式发生变 时，不需要重新生成代码 它还支持**强类型和模式进化**，其版本既向前兼容， 向后兼容。


### 主题和分区

kafka 的消息通过主题进行分类。主题就好比数据库的表，或者文件系统里的文件夹。主题可以被分为若干个分区 ， 一个分区就是一个提交日志。消息以追加的方式写入分区，然后以先入先出的顺序读取。由于一个主题一般包含几个分区，因此**无法在整个主题范围内保证消息的顺序，但可以保证消息在单个分区内的顺序**。

kafka 通过**分区来实现数据冗余和伸缩性**。分区可以分布在不同的服务器上，也就是说，一个主题可以横跨多个服务器，以此来提供比单个服务器更强大的性能。

在给定的分区里，每个消息的偏移量都是唯一的。消费者把**每个分区最后读取的消息偏移量保存在 Zookeeper Kafka 上**，如果消费者关闭或重启，它的读取状态不会丢失。

### broker和集群

一个独立的 Kafka 服务器被称为 broker broker 接收来自 生产者的消息，为消息设置偏移量，并提交消息到磁盘保存。 broker 为消费者提供服务，对读取分区的请求作出响应，返回已经提交到磁盘上的消息。

在集群中， 一个分区从属于一个broker，该broker被称为**分区的首领**。一个分区可以分配给多个broker ，这个时候会发生分区复制。这种复制机制为分区提供了消息冗余，如果有一个 broker失效，其他 broker 可以**接管领导权**。

> 分区复制和选举

主题可以配置自己的保留策略，可以将消息保留到不再使用它们为止。可以通过配置把主题当作紧凑型日志， 只有**最后一个带有特定键的消息**会被保留下来。

### 为什么选择kafka

- 多个生产者。无需协调来自不同生成者的数据流
- 多个消费者。kafka 也支持多个消费者从一个单独的消息流上读取数据，而且消费者之间直不影响。这与其他队列系统不同，其他队列系统的消息一旦被一个客户端读取，其他客户端就无法再读取它。另外，多个消费者可以组成一个群组，它们共享一个消息流，并保证整个群组对每个给定的消息只处理一次。
- 基于磁盘的数据存储。非实时读取消息，持久化。而**无需担心消息丢失或堵塞在生产者端** 消费者可以被关闭，但消息会继续保留在 Kafka 里。消费者可以从上次中断的地方继续处理消息。
- 伸缩性。一个具有灵活伸缩性的系统，对在线集群进行扩展丝毫不影响整体系统的可用性。要提高集群的容错能力，需要配置较高的复制系数。
- 高性能。上面提到的所有特性，让 Kafka 成为了一个高性能的发布与订阅消息系统。通过横向扩展生产者、消费者和 broker, Kafka 可以轻松处理巨大的消息流。在处理大量数据的同时，它还能保证亚秒级的消息延迟。

### 使用场景
- 活动追踪
- 传递消息
- 度量指标和日志记录
- 提交日志
- 流处理


P39: kafka的一些安装，实际简单例子和配置参数

### **如何选定分区数量**

原因： kafka 集群通过分区对主题进行横向扩展，所以当有新的broker 加入集群时，可以通过分区个数来实现集群的负载均衡。
当然，这并不是说，在存在多个主题的情况下（它们分布在多个 broker 上），为了能让分区分布到所有 broker 上， 主题分区的个数必须要大于 broker 的个数。不过，拥有大量消息的主题如果要进行负载分散，就需要大量的分区。

因素：

- 主题需要达到多大的吞吐量？例如，是希望每秒钟写入1OOKB 还是1GB?
- 从单个分区读取数据的最大吞吐量是多少？每个分区一般都会有一个消费者，如果你知道消费者将数据写入数据库的速度不会超过每秒50MB ，那么你也该知道，从一个分区读取数据的吞吐量不需要超过每秒 50MB 。
- 可以通过类似的方法估算生产者向单个分区写入数据的吞吐量，不过生产者的速度一般比消费者快得多，所以最好为生产者多估算一些吞吐量。
- 每个 broker 包含的分区个数、可用的磁盘空间和网络带宽。
- 如果消息是按照不同的键来写入分区的，那么为已有的主题新增分区就会很困难。
- 单个broker对分区个数是有限制的，**因为分区越多，占用的内存越多**，**完成首领选举需要的时间也越长**。

例子：很显然，综合考虑以上几个因素，你需要很多分区，但不能太多。如果你估算出主题的吞吐量和消费者吞吐量，可以用**主题吞吐量除以消费者吞吐量算出分区的个数**。也就是说，如果每秒钟要从主题上写入和读取 lGB 的数据，并且每个消费者每秒钟可以处理50MB的数据，那么至少需要 20 个分区。这样就可以让 20 个消费者同时读取这些分区，从而达到每秒钟 lGB 的吞吐量。

> 如果不知道这些信息，那么根据经验，把**分区的大小限制在25GB 以内** 可以得到比较理想的效果。


### 默认参数

根据时间保留数据是通过检查磁盘上日志片段文件的最后修改时间来实现的。


日志片段：
当消息到达 broker 时，它们被迫加到分区的当前日志片段上。当日志片段大小达到 log.segment bytes 定的上限（默认是 lGB ）时，**当前日志片段就会被关闭，一个新的日志片段被打开。如果一个日志片段被关闭，就开始等待过期**。这个参数的值越小，就会**越频繁地关闭和分配新文件，从而降低磁盘写入的整体效率**。


在日志片段被关闭之前消息是不会过期的。如果主题的消息量不大，那么如何调整这个参数的大小就变得尤为重要。例如，如果主题每天只接收 lOOMB 的消息，而 log.segment.bytes 使用默认设置，那么需要 10 天时间才能填满一个日志片段。


log.segment.ms指定了 **多长时间之后日志片段会被关闭**。


broker 通过设置 message.max.bytes 参数来**限制单个消息的大小**，默认值是1 000 000 ，也就是 1MB 。跟其他与字节相关的配置参数一样 ，该参数指的是压缩后的消息大小，也就是说，只要压缩后的消息小于 message.max.bytes 指定的值，消息的实际大小可以远大于这个值。


服务器端可用的**内存容量是影响客户端性能的主要因素**。磁盘性能影响生产者 ，而内存影响消费者.
消费者一般从分区尾部读取消息，如果有生产者存在，就紧跟在生产者后面。在这种情况下，消费者读取的消息会**直接存放在系统的页面缓存里，这比从磁盘上重新读取要快得多**。


### 需要多少个broker

- 首先，需要多少磁盘空间来保留数据，以及单个 broker 有多少空间可用。如果启用了数据复制，那么至少还需要一倍的空间。
- 集群处理请求的能力。这通常与网络接口处理客户端流量的能力有关，特别是当有多个消费者存在或者在数据保留期间流量发生波动（比如高峰时段的流量爆发）时。如果单个 broker 的网络接口在高峰时段可以达到80%的使用量，并且有两个消费者，那么消费者就无法保持峰值，除非有两个 broker。如果集群启用了复制功能，则要把这个额外的消费者考虑在内。因磁盘吞吐量低和系 内存不足造成的性能问题，也可以通过扩展多个 broker 来解决。


建议使用最新版本的 Kafka ，让消费者把偏移量提交到 Kafka 服务器上，消除对 **Zookeeper** 的依赖.


## 生产者


服务器在收到这些消息时会返回一个响应。如果消息成功写入 Kafka ，就返回RecordMetaData 对象，它包含了主题和分区信息，以及记录在分区里的偏移量。如果写入失败， 则 会返回一个错误。生产者在收到错误之后会尝试重新发送消息，几次之后如果还是失败， 就返回错误信息。



### 生产者的一些配置

到达副本数，内存缓冲区大小，压缩算法，重试，批次消息发送大小，批次等待时间、单个消息的最大值。TCP socke 接收和发送数据包的缓冲区大小


参考链接:

[kafka生产者](./kafka生产者.md)


---
## 序列化器

Avro 数据通过与语言无关 schema 来定义 schema 通过 JSON 来描述，数据被序列化成二进制文件或 JSON 文件，不过一般会使用二进制文件。 Avro。在读写文件时需要用到schema, schema一般会被内嵌在数据文件里。


用 Avro 的好处：我们修改了消息的 schema ，但并没有更新所有负责取数据的应用程序，而这样仍然不会出现异常或阻断性错误，也不需要对现有数据进行大幅更新。但有些兼容性原则。默认值？


我们把所有写人数据需要用到的 **schema 保存在注册表里**，然后在记录里引用 schema 的标识符。负责读取数据的应用程序**使用标识符从注册表里拉取 schema 来反序列化记录**。


## 分区

不过，一旦主题增加了新的分区，这些就无法保证了， 旧数据仍然留在旧的分区，但新的记录可能被写到其他分区上 。 如果要使用键来映射分区，那么最好在创建主题的时候就把分区规划好数量，而且**永远不要增加新分区**。

> 跟ES是一样的概念

---
# 消费者

Kafka 消费者从属于消费者群组。一个群组里的消费者订阅的是同 个主题，每个消费接收主题 部分分区的消息。


每个消费者只处理部分分区的消息，这就是横向伸缩的主要手段。我们有必要为主题创建大量的分区，在负载增长时可以加入更多的消费者。


一个新的消费者加入群组时，它读取的是原本由其他消费者读取的消息。当一个消费者被关闭或发生崩溃时，它就离开群组，原本由它读取的分区将由群组里的其他消费者来读取。分区的所有权从一个消费者转移到另一个消费者，这样的行为被称为**再均衡**。


在再均衡期间，消费者无法读取消息，造成**整个群组一小段时间的不可用**。另外，当分区被重新分配给另一个消费者时，消费者当前的读取状态会丢失，它有可能还需要去**刷新缓存**，在它重新恢复状态之前会拖慢应用程序。

消费者通过向被指派为**群组协调器**的 broker （不同的群组可以有不同的协调器）**发送心跳**来维持它们和群组的从属关系以及它们对分区的所有权关系。

在 0. 10.1 版本里， Kafka 社区引入了 一个独立的心跳线程，可以在轮均消息的空档发送心跳 。 这样一来，发送心跳的频率（也就是消费者群纽用于检测发生崩溃的消费者或不再发送心跳的消费者的时间）与消息轮询的频率（由处理消息所花费的时间未确定）之间就是**相互独立**的。在新版本的Kafka里，可以指定消费者在离开群纽并触发再均衡之前可以有**多长时间不进行消息轮询**，这样可以避免出现活锁(livelock），比如有时候应用程序并没有崩溃，只是由于某些原因导致无法正常运行。


正则表达式可以匹配多个主题， 如果有人创建了新的主题，并且主题的名字与正则表达式匹配，那么会立即触发再均衡，消费者就可以读取新添加的主题。

消息轮询是消费者 API的核心，通过个简单的轮询向服务器请求数据。一旦消费者订阅了主题，轮询就处理所有的细节，包括群组协调、分区再均衡、发送心跳和获取数据，开发者只需要使用一组简单的 API 来处理从分区返回的数据。


线程安全：按照规则， **一个消费者使用一个线程**

### 消费者配置

配置：最小发送大小，最小发送时间间隔，回话过期时间，提交偏移量的方式，分区分配策略

max.partition.fetch.bytes： 该属性指定了服务器从每个分区里返回给消费者的最大字节数。它的默认值是 lMB 。
（旧版本）消费者需要频繁调用 poll （）方法来避免会话过期和发生分区再均衡，如果单次调用 poll （） 返回的数据太多，消费者需要更多的时间 处理，可能无怯及时进行下 个轮询来避免会话过期。如果出现这种情况，以把 max.partition.fetch.bytes 值改 ，或者延长会话过期时间。

heartbeat.interval.ms： 指定了 poll （） 方住向协调发送心跳的频率， session.timeout. ms 则指定了消费者可以多久不发送心跳。所以，般需要同时修改这两个属性， heartbeat.interval.ms 必须比 session.timeout. ms 小，一般是 session.timeout. ms 三分之一 。

auto.offset.reset：该属性指定了消费者在读取一个没有偏移量的分区或者偏移量无效的情况下（因消费者长时间失效，包含偏移量的记录已经过时井被删除）该作何处理。


max.poll.records：单次调用 call （） 方住能够返回的记录数量，可以帮你控制在轮询里需要处理的数据量。


### 提交和偏移量

我们把**更新分区当前位置的操作叫作提交**。消费者往一个叫作 consumer_offset 特殊主题发送消息，消息里包含每个分区的偏移量。如果提交的偏移量小于客户端处理的最后一个消息的偏移量 ，那么处于两个偏移量之间的消息就会被重复处理。


假设我们仍然使用默认的 5s 提交时间间隔，在最近一次提交之后的 3s 发生了再均衡，再均衡之后，消费者从最后一次提交的偏移量位置开始读取消息。这个时候偏移量已经落后了 3s ，所以在这 **3s 内到达的消息会被重复处理**。可以通过**修改提交时间间隔来更频繁地提交偏移量**，减小可能出现重复消息的时间窗，不过这种情况是**无法完全避免的**。

> 间隔提交与再均衡导致的重复消费

在使用自动提交时，**每次调用轮询方法都会把上一次调用返回的偏移量提交上去**，它并不知道具体哪些消息已经被处理了，所以在再次调用之前最好确保所有当前调用返回的消息都已经处理完毕

大部分开发者通过控制偏移量提交时间来消除丢失消息的可能性，井在发生再均衡时**减少**(还是会存在的)重复消息的数量。消费者 API 提供了另一种提交偏移量的方式 ， 开发者可以**在必要的时候提交当前偏移盘**，而不是基于时间间隔。

CommitSync （） 将会提交由 poll （） 返回的最新偏移量 理完所有记录后要确保调用了 CommitSync（），否则还是会有丢失消息的风险。如果发生了再均衡，从最近一批消息到发生再均衡之间的所有消息都将被重复处理。手动提交有一个**不足之处**，在broker对提交请求**作出回应**之前，应用程序会一直**阻塞**，这样会**限制应用程序的吞吐量**。可以通过降低提交频率来提升吞吐量，但如果发生了再均衡，会**增加**重复消息的数量。

> 手动提交会相应之前阻塞（？自动提交就不会？因为自动提交是独立线程的？下面有异步方式），影响吞吐量，降低频率则会增加重复消息的数量

CommitAsync只管发送提交请求，无需等待 broker 的响应。为 CommitAsync （）也支持因调，在 broker作出响应时会执行回调。回调经常被用于**记录提交错误或生成度量指标**，不过如果你要用它来进行**重试**， 一定要**注意提交的顺序**。否则会出现重复消息。

在进行重试前，先检查**回调的序列号和即将提交的偏移量是否相等**，如果相等，说明没有新的提交，那么可以安全地进行重试。如果序列号比较大，说明有一个新的提交已经发送出去了，应该**停止重试**。

> 异步要注意重试提交的偏移量

#### 同步和异步组合提交：
一般情况下，针对**偶尔出现的提交失败，不进行重试不会有太大问题**，因为如果提交失因为临时问题导致的，那么**后续的提交总会有成功的**。但如果这是发生**在关闭消费者均衡前的最后一次提交**，就要**确保能够提交成功**。

提交特定的偏移量：消费者 API 允许在调用CommitSync（）和CommitAsync（）方法时传进去希望提交分区和偏移量的 map


再均衡监昕器：在为消费者分配新分区或移除旧分区 时，可以通过消费者API 执行 些应用程序代码，在调用 subscribe （） 方法时传进去 Consume Rebalancelistener 「实例就可以了。Consume Rebalancelistener 「有两个需要实现的方法。

### ***原子性操作***
如果保存记录和偏移量可以在一个原子操作里完成，就可以避免出现上述情况。记录和偏移量要么都被成功提交，要么都不提交。如果记录是保存在数据库里而偏移量是提交到Kafka 上，那么就无法实现原子操作。

不过 ，如果**在同一个事务里把记录和偏移量都写到数据库**里会怎样呢？那么我们就会知道记录和偏移量要么都成功提交，要么都没有，然后重新处理记录。

现在的问题是 ：如果偏移量是保存在数据库里而不是 Kafka 里，那么消费者在得到新分区时怎么知道该从哪里开始读取？这个时候可以使用**seek（）方法**。在消费者启动或分配到新分区时 ，可以使用 seek （）方告查找保存在数据库里的偏移量。

> 假设偏移量不是写在mysql上的场景：mysql 等待偏移量的提交结果，但是因为网络问题，虽然偏移量已经提交了，但是**程序没有收到**，这时候**数据回滚**。但是因为偏移量已经提交了，导致继续消费下一批数据，造成数据的丢失。

### 如何退出

如果确定要退出循环，需要通过另一个线程调用 consume.wakeup（）方住。如果循环运行在主线程里，可以在 ShutdownHook 里调用该方陆。要记住， consume.wakeup（） 是消费者唯一一个可以从其他线程里安全调用 的方法。调用 consume.wakeup（） 可以退出 poll() ,并抛出 WakeupException 异常，或者如果调用 consume.wakeup（）时线程没有等待轮询， 那么异常将在下一轮调用 poll （）时抛出。我们不需要处理 Wakeup Exception ，因为它只是用于跳出循环的一种方式。不过 ， 在**退出线程之前调用 consume.close （） 是很有必要的**， 它会**提交任何还没有提交的东西** ， 并向群组协调器发送消息，告知自己要离开群组，接下来就会**触发再均衡** ，而不需要**等待会话超时**。

> 生产环境中遇到类似的问题

一个 消费者可以订阅主题（井加入消费者群组），或者为自己分配分区 但不 时做这两件事情。

# 深入kafka

kafka 使用 Zoo keeper 来维护集群成员的信息。每个 broker 都有一个唯一标识符，这个标识符可以在配置文件里指定，也可以自动生成。在broker启动的时候，它通过创建临时节点把自己的ID注册到Zookeeper。Kafka组件订阅Zookeeper的/brokers/ids路径(broker在Zookeeper上的注册路径），当有broker加入集群或退出集群时，这些组件就可以获得通知。

## 控制器

控制器其实就是一个broker，只不过它除了具有一般broker的功能之外，还负责**分区首领的选举**。

集群里第一个启动的broker通过在Zookeeper里创建一个临时节点/controller让自己成为控制器。其他broker在启动时也会尝试创建这个节点，不过它们会收到一个“节点已存在”的异常，然后“意识”到控制器节点已存在，也就是说集群里已经有一个控制器了。其他broker在**控制器节点上创建Zookeeper watch对象**，这样它们就可以收到**这个节点的变更通知**。这种方式可以**确保集群里一次只有一个控制器存在**。

集群里的其他broker通过watch对象得到控制器节点消失的通知，它们会**尝试让自己成为新的控制器**。**第一个**在Zookeeper里成功创建控制器节点的broker就会成为新的控制器，其他节点会收到“节点已存在”的异常，然后在新的控制器节点上再次创建watch对象。每个新选出的控制器通过Zookeeper的**条件递增操作获得一个全新的、数值更大的controller epoch**。其他broker在知道当前controller epoch后，如果收到由控制器发出的**包含较旧epoch的消息，就会忽略它们**。

> 故障重新选举后通过版本号忽略旧的消息

当控制器发现一个broker已经离开集群（通过观察相关的Zookeeper路径），它就知道，那些**失去首领的分区需要一个新首领**（这些分区的首领刚好是在这个broker上）。控制器遍历这些分区，并确定谁应该成为新首领（简单来说就是分区副本列表里的下一个副本），然后向**所有包含新首领或现有跟随者的broker发送请求**。该请求消息包含了谁是新首领以及谁是分区跟随者的信息。随后，新首领开始处理来自**生产者和消费者的请求**，而**跟随者开始从新首领那里复制消息**。

当控制器发现一个broker加入集群时，它会使用broker B来检查新加入的broker是否包含现有分区的副本。如果有，控制器就把变更通知发送给新加入的broker和其他broker,新broker上的副本开始从首领那里复制消息。

简而言之，Kafka使用Zookeeper的临时节点来选举控制器，并在节点加入集群或退出集群时通知控制器。控制器负责在节点加入或离开集群时进行**分区首领**选举。控制器使用**epoch来避免“脑裂”**。“脑裂”是指两个节点同时认为自己是当前的控制器。

## 复制

Kafka使用主题来组织数据，每个主题被分为若干个分区，每个分区有多个副本。那些副本被保存在broker上，每个broker可以保存成百上千个属于不同主题和分区的副本。

- 首领副本。每个分区都有一个首领副本为了保证一致性，所有**生产者请求和消费者请求都会经过这个副本**。
- 跟随者。副本首领以外的副本都是跟随者副本。**跟随者副本不处理来自客户端的请求**，它们唯一的任务就是从首领那里**复制消息，保持与首领一致的状态**。如果首领发生崩渍，其中的跟随者会被提升为新首领。

> 首领跟跟随者的作用。副本并不能分担负载。

首领的另一个任务是**搞清楚哪个跟随者的状态与自己是一致的**。如果跟随者发送了请求消息4，那么首领就知道它已经收到了前面3个请求的响应。通过查看每个跟随者请求的最新偏移量，首领就会知道每个跟随者复制的进度。如果跟随者在10内没有请求任何消息，或者虽然在请求消息，但在10s内没有请求最新的数据，那么它就会被认为是不同步的。


**持续请求得到的最新消息副本被称为同步的副本**。在首领发生失效时，**只有**同步副本才有可能被选为新首领。


除了当前首领之外，每个分区都有一个**首选首领**创建主题时选定的首领就是分区的首选首领。之所以把它叫作首选首领，是因为在创建分区时，需要在broker之间均衡首领（后面会介绍在broker间分布副本和首领的算怯）。因此，我们希望首选首领在成为真正的首领时，broker间的负载最终会得到均衡。


**Kafka客户端要自己负责把生产请求和获取请求发送到正确的broker上**。元数据请求可以发送给任意一个broker，因为所有broker都缓存了这些信息。刷新的时间间隔通过metadata.max.age.Ms参数来配置

如果客户端收到“非首领”错误，它会在尝试重发请求之前**先刷新元数据**，因为这个错误说明了客户端正在使用过期的元数据信息，之前的请求被发到了错误的broker上。

请求需要先到达指定的分区首领上，然后客户端通过查询元数据来确保请求的路由是正确的。

> 刷新元数据，确保发送到正确的broker首领。

### 获取请求

零复制: Kafka 使用零复制技术向客户端发送消息一一也就是说， Kafka 直接把消息从文件（或者更确切地说是**Linux文件系统缓存）里发送到网络通道，而不需要经过任何中间缓冲区**。其他数据库在将数据发送给客户端之前会先把它们保存在本地缓存里。这项技术避免了字节复制，也不需要管理内存缓冲区，从而获得更好的性能。

定时定量发送，减少CPU和网络开销

并不**是所有保存在分区首领上的数据都可以被客户端读取**,因为**还没有被足够多副本复制的消息**被认为是“不安全”的,如果broker间的**消息复制因为某些原因变慢，那么消息到达消费者的时间也会随之变长**。

## 物理存储
分区分配测量，计算每个目录里的分区数量，**新的分区总是被添加到数量最小的那个目录里**。也就是说，如果添加了一个新磁盘，所有新的分区都会被创建到这个磁盘上。因为在完成分配工作之前，新磁盘的分区数量总是最少的。

> 新的分区总是被添加到数量最小的那个目录里

### 文件管理
因为在一个大文件里查找和删除消息是很费时的，也很容易出错，所以我们把**分区分成若个片段**。默认情况下，每个片段包含lGB或者1周的数据，以较小的那个为准。在 broker往分区写入数据时，如果达到片段上限，就关闭当前文件，井打开一个新文件。当前正在写入数据的片段叫作**活跃片段**。活动片段**永远不会被删除**。

### 文件格式

我们把 Kafka 的消息和偏移量保存在文件里。保存在磁盘上的数据格式与从生产者发送过来或者发送给消费者的消息格式是**一样**的。因为**使用了相同的消息格式进行磁盘存储和网络传输** ， Kafka 可以使用**零复制技术给消费者发送消息，同时避免了对生产者已经压缩过的消息进行解压和再压缩**。

除了键、值和偏移量外， 消息里还包含了消息大小、校验和、消息格式版本号、压缩算能(Snappy 、 GZip 或 LZ4 ）和时间戳（在 0.10.0 版本里引入的）。时间戳可以是生产者发送消息的时间，也可以是消息到达 broker 的时间，这个是可配置的。

如果生产者发送的是压缩过的消息，那么同一个批次的消息会被压缩在一起，被当作“包装消息”进行发送。于是， broker 就会收到一个这样的消息，然后再把它发应给消费者。消费者在解压这个消息之后，会看到整个批次的消息，它们都有自己的时间戳和偏移量。

也就是说，如果在生产者端使用了压缩功能（极力推荐），那么发送的批次越大，就意味着在网络传输和磁盘存储方面会获得越好的压缩性能，同时意味着如果修改了消费者使用的消息格式（例如，在消息里增加了时间戳），那么网络传输和磁盘存储的格式也要随之修改，而且 broker 要知道如何处理包含了两种消息格式的文件。


### 索引

为了帮助 broker 更快地定位到指定的偏移量， Kafka为每个分区维护了**一个索引**。索引把**偏移量映射到片段文件和偏移量在文件里的位置**。

索引也被分成片段，所以在删除消息时，也可以删除相应的索引Kafka不维护索引的校验和。如果索引出现损坏，Kafka会通过**重新读取消息并录制偏移量和位置来重新成索引**。如果有必要，管理员可以删除索引，这样做是绝对安全的， Kafka 会自动重新生成这些索引。


### 清理

在应用程序从崩溃中恢复时，它从Kafka 读取消息来恢复最近的状态。在这种情况下，应用程序只关心它在崩溃前的那个状态，而不关心运行过程中的那些状态。

- 干净的部分:这些消息之前被清理过，每个键只有一个对应的值，这个值是上一次清理时保留下来的。
- 污浊的部分:这些消息是在上一次清理之后写入的。


为了清理分区 ，清理线程会读取分区的污浊部分，井在内存里创建 map。map 里的 每个元素 含了消息键的散列值和消息的偏移量，键的散列值是 16B ，加上偏移量总共是24B 如果要清理 lGB 的日志片段，并假设每个消息大小为1kb，那么这个片段就包 含一百万个消息，而我们只需要用 24MB map 就可以清理这个片段。（如果有重复的键，可以重用散列项，从而使用更少的内存。）这是非常高效的！

清理线程在创建好偏移盐 map 后，开始从干净的片段处读取消息，从最旧的消息开始，把它们的内容与 map 里的内容进行比对。它会检查消息的键是否存在于map 中，如果不存在，那么说明消息的值是最新的，就把消息复制到替换片段上。 如果键已存在，消息会被忽略，因为在分区的后部已经有一个具有相同键的消息存在。在复制完所有的消息之后，我们就将替换片段与原始片段进行交换，然后开始清理下一个片段。


为了彻底把一个键从系统里删除，应用程序必须发送一个包含该键且值为 null 的消息。清理线程发现该消息时，会先进行常规的清理，只保留值为 null 的消息。该消息（被称为**墓碑消息**）会被保留一段时间，时间长短是可配置的。

在这个时间段过后，清理线程会移除这个墓碑消息，这个键也将从 Kafka 分区里消失。重要的是，要留给消费者足够多的时间，让他看到墓碑消息，因为如果消费者离线几个小时并错过了墓碑消息，就看不到这个键，也就不知道它已经从 Kafka 里删除，从而也就不会去删除数据库里的相关数据了。


### 墓碑消息

为了彻底把一个键从系统里删除，应用程序必须发送 个包含该键且值为 null 的消息。清理线程发现该消息时，会先进行常规的清理，只保留值为 null 的消息。该消息（被称为墓碑消息）会被保留一段时间，时间长短是可配置的。

在这个时间段过后，清理线程会移除这个墓碑消息，这个键也将从 Kafka 分区里消失。重要的是，要留给消费者足够多的时间，让他看到墓碑消息，因为如果消费者离线几个小时并错过了墓碑消息，就看不到这个键，也就不知道它已经从 Kafka 里删除，从而也就不会去删除数据库里的相关数据了。


[](#bookmark)
# 可靠的数据传递

## 可靠性保证
Kafka 可以在哪些方面作出保证呢？
- Kafka 可以保证**分区消息的顺序**。
- 只有当消息被写入分区的所有**同步副本**时（但不一定要写入磁盘），它才被认为是“提交”的
- 只要还有一个副本是活跃的，那么已经提交的消息就不会丢失
- 消费者只能读取已经提交的消息。

这种权衡一般是指消息存储的可靠性和一致性的重要程度与可用性、高吞吐量、低延迟和硬件成本的重要程度之间的权衡。


## 复制
分区首领是同步副本，而对于跟随者副本来说，它需要满足以下条件才能被认为是同步的。
- 与 Zookeeper 之间有 一 个活跃的会话，也就是说，它在过去的6s（可配 置）内向Zoo keeper 发送过心跳。
- 在过去的 10s 内（可配置）从首领那里获取过消息。
- 在过去的 10s 内从首领那里获取过最新的消息。光从首领那里获取消息是不够的，它还必须是儿乎零延迟的。

> 所以如果ack设为1，只需要首领副本的回复，还是会造成消息的丢失的，因为判定一个副本不同步需要一小段时间

## broker配置

### 复制系数

多个副本在同步和非同步状态之间快速切换，说明集群内部出现了问题，通常是 Java 不恰当的垃圾回收配置导致的。不恰当的垃圾回收配置会造成几秒钟的停顿，从而让 broker 与 Zookeeper 之间断开连接，最后变成不同步的，进而发生状态切换。

简而言之，如果我们**允许不同步的副本成为首领，那么就要承担丢失数据和出现数据不一致的风险**。 如果不允许它们成为首领，那么就要接受较低的可用性，因为我们必须等待原先的首领恢复到可用状态。

### 不完全的首领选举
如果把 unclean.leader.election.enable 设为 true ，就是**允许不同步的副本成为首领**（也就是“ 不完全的选举勺，那么我们将面临丢失消息的风险。如果把这个参数设为false ,就要等待原先的首领重新上线，从而降低了可用性。我们经常看到一些对数据质量和数据一致性要求较高的系统会禁用这种不完全的首领选举（把这个参数设为 false ） 。银行系统是这方面最好的例子，大部分银行系统宁愿选择在几分钟甚至几个小时内不处理信用卡支付事务，也不会冒险处理错误的消息。不过在对可用性要求较高的系统里，比如实时点击流分析系统， 一般会启用不完全的首领选举。

> Kafka 0.11.0.0版本开始 unclean.leader.election.enable 参数的默认值由原来的true 改为false

### min.insync.replicas

最少同步副本，默认值1。


## 在可靠的系统里使用生产者

1. 禁用了不完全首领选举， 所以如果**ack设为1**（默认值），只需要首领副本的回复，**还是会造成消息的丢失的**，因为判定一个副本不同步需要一小段时间

2. 禁用了不完全首领选举， 所以如果**ack设为all**, 假设现在往 Kafka 发送消息，**分区的首领刚好崩溃，新的首领正在选举当中**， Kafka 会向生产者返回“首领不可用”的响应。 在这个时候，如果生产者没能**正确处理这个错误，也没有重试发送消息直到发送成功**，那么消息也有可能丢失。

可靠性保持：
- 根据可靠性需求**配置恰当**的 acks 值。
- 在参数配置和代码里**正确处理错误**。

办法：
- 发送确认.
- 配置生产者的重试参数。重试和恰当的错误处理**可以保证每个消息“至少被保存一次”**，但当前的 Kafka 版本（ 0.10.0 ）**无法保证每个消息“只被保存一次”**。现实中的很多应用程序在消息里加入唯一标识符，用于检测重复消息，**消费者在读取消息时可以对它们进行清理**。
- 额外的错误处理。不可重试的 broker 错误，例如消息大小错误、认证错误等；在消息发送之前发生的错误，例如序列化错误；在生产者达到重试次数上限时或者在消息占用的内存达到上限时发生的错误。

> ?: 为什么是至少被保存一次，场景与原因分析？重试问题：如果发送成功后，kafka保存数据成功，只是ack没收到，重试则会发送重复数据。

> 无法保证每个消息“只被保存一次”，还是很困难的。消息加入唯一标识符。消费时进行清理。幂等性

## 在可靠的系统里使用消费者

消费者唯一要做的是跟踪哪些消息是已经读取过的，哪些是还没有读取过的。这是在读取消息时不丢失消息的关键。

4 个非常重要的配置参数：
- group.id
- auto.offset.reset: earliest、latest
- enable.auto.commit
- auto.commit.interval.ms

### 显式提交偏移量

- 总是在处理完事件后再提交偏移量
- 提交频度是性能和重复消息数量 之间的权衡
- 确保对提交的偏移量 心里有数。轮询过程中提交偏移量有一个不好的地方，就是提交的偏移量有可能是**读取到的最新偏移量，而不是处理过的最新偏移量**。在**处理完消息后再提交偏移量**是非常关键的 否则会导致消费者错过消息。第四章
- 再均衡。一般要在分区被撤销之前提交偏移量，井在分配到新分区时清理之前的状态。
- 消费者可能需要重试。
  - 第一种模式，在遇到可重试错误时，提交最后一个处理成功的偏移量，然后把还没有处理好的消息保存到缓冲区里（这样下一个轮询就不会把它们覆盖掉），**调用消费者的 pause()方法来确保其他的轮询不会返回数据**（不需要担心在重试时缓冲区隘出），在保持轮询的同时尝试重新处理（关于为什么不能停止轮询，请参考第 4 章）。如果重试成功，或者重试次数达到上限井决定放弃，那么把错误记录下来井丢弃消息，然后调用 resume（）方能让消费者继续从轮询里获取新数据。
  - 第二种模式，在遇到可重试错误时，**把错误写入一个独立的主题，然后继续**。 一个独立的消费者群组负责从该主题上读取错误消息，井进行重试，或者使用其中的一个消费者同时从该主题上读取错误消息并进行重试，不过在重试时需要暂停该主题。这种模式有点像其他消息系统里的 dead-letter-queue。
- 消费者可能需要**维护状态**。你不应该尝试自己去解决这个问题，建议尝试一下 **KafkaStreams** 这个类库 ，它为聚合、连接、时间窗和其他复杂的分析提供了高级的 DSLAPI。
- 长时间处理。旧版本心跳管理。
- **仅一次传递**。
  - 尽管 Kafka 现在**还不能完全支持仅一次语义**。，要么消息本身包含一个唯一键（通常都是这样），要么使用主题、分区和偏移量的组合来创建唯唯一键－它们的组合可以唯一标识Kafka记录。如果你把消息和一个唯一键写入系统，然后碰巧又读到个相同的消息，只要把原先的键值覆盖掉即可。**幂等性写入**
  - 如果写入消息的系统支持事务， 那么就可以使用另一种方法。**我们把消息和偏移量放在同一个事务里 ，这样它们就能保持同步**。在消费者启动时，它会获取最近处理过的消息偏移量 ，然后调用 seek（）方也从该偏移量位置继续读取数据。

> 仅一次传递。幂等性

----
## 验证系统可靠性

Kafka 的代码库里包含了大量测试用例，考虑运行以下一些测试。[](#bookmark)

- 首领选举：如果我停掉首领会发生什么事情？生产者和消费者重新恢复正常状态需要长时间？
- 控制器选举: 重启控制器后系统需要多少时间来恢复状态？
- 依次重启：可以依次重启 broker 不丢失任何数据吗？
- 不完全首领选举测试：如果依次停止所有副本（确保每个副本都变为不同步的），然启动一个不同步的broker会发生什么？要怎样恢复正常？这样做是可接受的吗？

### 在生产环境监控可靠性

对于生产者来说，最重要的两个可靠性指标是消息的 error-rate retry-rate （聚合过的）。

对于消费者来说，最重要的指标是 consumer lag ，该指标表一了**消费者的处理速度与最近提交到分区里的偏移量之间还有多少差距**。理想情况下，该指标总是为 ，消费者总能读到最新的消息。


---
# 数据管道

Kafka 为数据管道带来的主要价值在于，它可以作为数据管道各个数据段之间的大型缓冲区， 有效地解耦管道数据的生产者和消费者。 Kafka 的解藕能力以及在安全和效率方面的可靠性，使它成为构建数据管道的最佳选择。

数据管道需要协调各种数据格式和数据类型。

数据管道的构建可以分为两大阵营，即 ETL ELT。

ETL当数据流经数据管道时，数据管道会负责处理它们。这种方式为我们节省了时间和存储空间，因为不需要经过保存数据、修改数据、再保存数据这样的过程。有可能给数据管道造成不适当的计算和存储负担。下游得到的数据不是完整的，如果它们想要访问被移除的字段，只能重新构建管道，井重新处理历史数据（如果可能的话）。


ELT：在这种模式下，数据管道只做少量的转换（主要是数据类型转换），确保到达数据地的数据尽可能地与数据源保持一致。这种情况也被称为高保真（ high fidelity ）数据管道或数据湖（data lake ）架构。目标系统收集
“原始数据”，并负责处理它们。这种方式为目标系统的用户提供了最大的灵活性，因为它们可以访问到完整的数据。在这些系统里诊断问题也变得更加容易，因为数据被集中在同一个系统里进行处理，而不是分散在数据管道和其他应用里。这种方式的不足在于，数据的转换占用了目标系统太多的 CPU 和存储资源。有时候，目标系统造价高昂，如果有可能，人们希望能够将计算任务移出这些系统。


连接器和任务负责“数据的移动”， worker 进程负责 REST API 、配置管理、可靠性、高可用性、伸缩性和负载均衡。

Connect API的好处： 编写代码从 Kafka 读取数据并将其插入数据库只需一到两天的时间，但是如果要处理好配置、异常、 REST API 、监控、部署、伸缩、失效等问题，可 需要几个月，如果你使用连接器来实现数据复制，连接器插件会为你处理掉大堆复杂的问题。


源连接器所做的事情都很相似一一－ 系统读 事件，并为每个事件生成schema 和值（值就是数据对象本身）
目标连接器正好相反，它们获取 schema 和值，井使schema 来解析值，然后入到目标系统。
中间可以选择使用合适的转化器，用于将数据保存到 Kafka。

在设计一个源连接器时，要着重考虑如何对源系统的数据进行分区以及如何跟踪偏移量，这将影响连接器的井行能力，也决定了连接器是否能够实现至少一 次传递或者仅一次传递。

可靠性是数据集成系统唯一一个重要的需求

---
# 跨集群数据镜像

### Hub Spoke

一个中心 Kafka 集群对应多个本地Kafka集群的情况。

场景：当消费者需要访问的数据集分散在多个数据中心时，可以使用这种架构。如果每个数据中心的应用程序只处理自己所在数据中心的数据，那么也可以使用这种架构，只不过它们无法访问到全局的数据集。

这种架构的好处在于：

- 数据只会在本地的数据中心生成，而且每个数据中 心的数据只会被镜像到中央数据中心一次。
- 因为数据复制是单向的，而且消费者总是从同一个集群读取数据，所以这种架构易于部署、配置和监控。

坏处：
- 这种架构模式在数据访问方面有所局限，因为区域数据中心之间的数据是完全独立的。
- 在采用这种架构时，每个区域数据中心的数据都需要被镜像到中央数据中心上。镜像进程会读取每 个区域数据中心的数据，并将它重新生成到中心集群上。如果多个数据中心出现了重名的主题，那么这些主题的数据可以被写到中心集群的单个主题上 ，也可以被写到多个主题上。

### 双活架构
这种架构的主要好处在于：
- 它可以为就近的用户提供服务，具有性能上的优势，而且不会因为数据的可用性问题（在 Hub Spoke架构中就有这种问题）在功能方面作出牺牲。
- 第二个好处是冗余和弹性。因为每个数据中心具备完整的功能，一个数据中心发生失效，就可以把用户重定向到另一个数据中心。这种重定向完全是网络的重定向，因此是一种最简单、最透明的失效备援方案。

这种架构的主要问题在于，如何在进行多个位置的数据异步读取和异步更新时避免冲突。

双活镜像（特别是当数据中心的数量超过两个）的挑战之处在于：
- 每两个数据中心之间需要进行镜像，而且是双向的。
- 另外，我们还要避免循环镜像，相同的事件不能无止境地来回镜像。

如果能够很好地处理在从多个位置异步读取数据和异步更新数据时发生的冲突问题，那么我们强烈建议使用这种架构。这种架构是我们所知道的最具伸缩性、弹性、灵活性和成本优势的解决方案。所以，它值得我们投入精力去寻找一些办怯，用于避免循环复制、把相同用户的请求粘在同一个数据中心，以及在发生冲突时解决冲突。


#### 主备架构
冷备。你可以安装第二个集群，然使用镜像进程将第一个集群的数据完整镜像到第二个集群上，不需要担心数据的访问和冲突问题，也不需要担心它会带来像其他架构那样的复杂性。

这种架构的不足在于，它浪费了一个集群。 Kafka 集群间的失效备援比我们想象的要**难得多**。从目前的情况来看，要实现不丢失数据或无重复数据的 Kafka 集群失效备援是不可能。

有些组织则倾向于让灾备集群在平常也能发挥作用，他们把些只读的工作负载定向到灾备集群上，也就说，实际上运行的是 Hub Spoke 架构的一个简化版本，因为架构里只有一个Spoke。


目前 Kafka 不支持事务，那么在失效备提过程中，一些数据可以及时到达灾备集群，而有些则不能。那么在切换到灾备集群之后，应用程序需要知道该如何处理没有相关销售信息的产品数据。


在切换到灾备集群的过程中，最具挑战性的事情莫过于如何让应用程序知道该从什么地方开始继续处理数据：

- 要么从头开始取数据，并处理大量的重复数据，要么直接跳到末尾，放弃一些数据（希望只是少量数据）
复制偏移量主题，，消费者会把偏移量提交到一个_consurner_offsets 的主题上。如果对这个主题进行了镜像。首先， 我们并不能保证主集群里的偏移量与灾备集群里的偏移量是完全匹配 。即使如此生产者在后续进行重试时仍然会造成偏移量的偏离。简而言之，目前的 Kafka 镜像解决方案无怯为主集群和灾备集群保留偏移量。
- 用于根据时间戳查找偏移量。于是，假设你正在进行失效备援 井且知道失效事件发生在凌 4:05 ，那么就可以让消费或者从 4: 03 的位置开始处理数据。
- 我们知道，镜像偏移量主题的一个最大问题在于主集群和灾备集群的偏移量会发生偏差。因此，一些组织选择使用外部数据存储（比如 Apache Cassandra 来保存集群之间的偏移量映射。这种方案非常复杂，我认为并不值得投入额外的时间。

但在今天，我倾向于将集群升级到新版本，并使用基于时间戳的解决方案，而不是进行偏移量映射，更何况偏移量映射并不能覆盖所有的失效备援场景。

最简单的解决方案是清理旧的主集群，删掉所有的数据和偏移量，然后从新 主集群上把数据镜像回来，这样可以保证两个集群的数据是一致的。


首先，延展集群井非多个集群，而是单个集群，因此不需要对延展集群进行镜像。延展集群使用 Kafka 内置的复制机制在集群的broker 之间同步数据。

这种架构的不足之处在于，它所能应对的灾难类型很有限，只能对数据中心的故障，无法应对应用程序或者 Kafka 故障。运维的复杂性是它的另 个不足之处，它所 要的物基础设施并不是所有公司都能够承担得起的。


MirrorMaker 为每个消费者分配一个线程，消费集群的主题和分区上读取数据，然后通过公共生产者将数据发送到目标集群上


如果说broker 只有一个可监控的度量指标，那么它 定是指非同步分区的数量。 该度量指明了作为首领的broker 有多少个分区处于非同步状态。


如果多个 broker 都出现了非同步分区， 那么有可能是集 的问题，也有可能是单个 broker的问题。这时候有可能是因为 broker 无陆从其 broker 那里复制数据。为了找出这个、broker ，可以列出集群的所有非同步分区 ，井检查它们的共性.

- 网络输入吞吐量。
- 网络输出吞吐量。
- 磁盘平均等待时间。
- 磁盘使用百分比。

任何时候，都应该只有 broker 是控制器，而且这个 broker必须一直是集群控制器。如果出现了两个控制器，说明有 个本该退出的控制器线程被阻塞了，这会导致管理任务无陆正常执行，比如移动分区。

请求处理器平均空闲百分比这个度量指标表示请求处理器空闲时间的百分比。数值越低，说明 broker 的负载越高。


第一种是 kafka.controller，可以将它设置为 INFO 。这个日志     用于记录集群控制器的信息。在任何时候，集群里都只有一个控制器，因此只有一个 broker会使用这个日志。日志里包含了主题的创建和修改操作、 broker 状态的变更，以及集群的活动，比如默认的副本选举和分区的移动。

另一个日志是 kafka.se ver.ClientQuotaManager也可以将它设置为 INFO 级别。这个日志用于记录与生产和消费配额活动相关的信息。因为这些信息很有用，所以最好不要把它们记录在 broker 的主日志文件里。

我们可以对 equest-latency avg 设置告警，它表示发送一个生产者请求到 broker 所需要的平均时间。

# 流式处理


事件流的特点：

- 事件流是有序
- 不可变的数据记录:事件一旦发生，就不能被改变。个金融交易被取消，并不是说它就消失了，相反，这需要往事件流里添 个额外的事件，表示前 个交易的取消操作。
- 事件流是可重播的,，但对于大多数业务来说，重播发生在几个月前（甚至几年前）的原始事件流是个很重要的需求。可能是为了尝试使用新的分析方法纠正过去的错误，或是为了进行审计。

流式处理的三种范式：
- 请求与响应。这是延迟最小的一种范式，响应时间处于亚毫秒到毫秒之间，而且响应时间一般非常稳定。这种处理模式一般是阻塞的，应用程序向处理系统发出请求，然后等待响应。
- 这种范式具有高延迟和高吞吐量的特点。处理系统按照设定的时间启动处理进程。它们每天加载巨大批次的数据，井生成报表，用户在下一次加载数据之前看到的都是相同的报表
- 流式处理。部分的业务不要求亚毫秒级的响应，不过也接受不了要等到第二天才知道结果。比如针对可疑信用卡交易的警告、网络警告、根据供应关系实时调整价格、跟踪包衷

### 时间

- 事件件时间
- 日志追加时间
- 处理时间

> 注意时区问题

### 状态

如果操作里包含了多个事件，流式处理就会变得很有意思，比如根据类型计算事件的数量、移动平均数、合并两个流以便生成更丰富的信息流。比如这个小时内看到的每种类型事件的个数、需要合井的事件、将每种类型的事件值相加等等。事件与事件之间的信息被称为“状态“。


这些状态一般被保存在应用程序的本地变量里。例如，使用散列表来保存移动计数器。不过，这不是一种可靠的方搓，因为如果应用程序关闭，状态就会丢失，结果就会发生变化，而这并不是用户希望看到的。所以，要小心地持久化最近的状态，如果应用程序重启，要将其恢复。

- 本地状态或内部状态
这种状态只能被单个应用程序实例访问，它们一般使用内嵌在应用程序里的数据库进行维护和管理。本地状态的优势在于它的速度，不足之处在于它受到内存大小的限制。

- 外部状态
这种状态使用外部的数据存储来维护， 一般使用 NoSQL 系统。使用外部存储的优势在于，它没有大小的限制，而且可以被应用程序的多个实例访问，甚至被不同的应用程序访问。不足之处在于，引人额外的系统会造成更大的延迟和复杂性。大部分流式处理应用尽量避免使用外部存储，或者将信息缓存在本地，减少与外部存储发生交互，以此来降低延迟，而这就引入了如何维护内部和外部状态一致’性的问题。


### 流和表的二元性

流包含了变更一一流是一系列事件，每个事件就是一个变更。表包含了当前的状态，是多个变更所产生的结果。


如果能够捕捉数据库的变更事件，井形成事件流，流式处理作业就可以监昕 件流， 井及时更新缓存。捕捉数据库的变更事件井形成事件流，这个过程被称为 CDC 变更数据捕捉（Change Data Capture）。

为了将流转化成表， 需要“应用”流里所包含的所有变更，这也叫作流的“物化”。首先在 内存里 内部状态存储或外部数据库里创建 个表，然后从头到尾遍历流里的所有事件，逐个地改变状态。在完成这个过程之后，得到了个表，它代表了某个时间点的状态。

### 时间窗口
如果“移动间隔”与窗口大小相等，这种情况被称为“滚动窗口”（tumbling window ）”。如果窗口随着每一条记录移动，这种情况被称为“滑动窗口（sliding dow ）”。跳跃窗口，每分钟统计近5分钟的数据。


### 使用本地状态

大部分流式处理应用程序关心的是如何聚合信息，特别是基于时间窗口进行聚合。

要实现这些聚合操作，需要维护流的状态。

如果流式处理应用程序包含了本地状态，情况就会变得非常复杂，而且还需要解决下列的一些问题。

- 内存使用: 应用实例必须有可用的内存来保存本地状态。
- 持久化： 要确保在应用程序关闭时不会丢失状态，并且在应用程序重启后或者切换到另一个应用实例时可以恢复状态。
- 再均衡： 有时候，分区会被重新分配给不同的消费者。在这种情况下，失去分区的实例必须把最后的状态保存起来 ， 同时获得分区的实例必须知道如何恢复到正确的状态。

### 多阶段处理和重分区

处理每个 reduce 步骤的应用需要被隔离开来。与 Map-Reduce 不同的是，大多数流式处理框架可以将多个步骤放在同一个应用里 ，框架会负责调配每一步需要运行哪一个应用实例


### 使用外部查找一一流和表的连接

有时候，流式处理需要将外部数据和流集成在起，比如使用保存在外部数据库里的规则来验证事务，或者将用户信息填充到点击事 中。
这种方式最大的问题在于，外部查找会带来严重的延迟。

为了获得更好的性能和更强的伸缩性，需要将数据库的信息缓存到流式处理 用程序里。不过，要管理好这个缓存也是一个挑战。

如果刷新太频繁，那么仍然会对数据库造成压力，缓存也就失去了作用。如 刷新不及时， 那么流式处理中所用的数据就会过时。

### 流与流的连接

不过 ，如果要连接两个流，那么就是在连接所有的历史事件一一将两个流里具有相同键和发生在相同时间窗口内的事件匹配起来。这就是为什么流和流的连接也叫作基于时间窗口的连接

在 Streams 中，上述的两个流都是通过相同的键来进行分区的，这个键也是用于连接两个流的键。

### 乱序的事件如何处理

- 它们在本地状态里维护了多个聚合时间窗口，用于更新事件，并为开发者提供配置时间窗口大小的能力。当然，时间窗口越大，维护本地状态需要的内存也越大。

- Streams  API 通常将聚合结果写到主题上。这些主题一 般是压缩日志主题，也就是说，它们只保留每个键的最新值。如果一个聚合时间窗口的结果需要被更新为晚到事件的结果，Streams 会直接为这个聚合时间窗口写入一个新的结果，将前一个结果覆盖掉。

窗口的可更新时间是多长。理想情况下，可以定义一个时间段，在这个时间段内， 事件可以被添加到与它们相应的时间片段里。如果事件处于 4个小时以内，那么就更新它们就忽略它们。

### 重新处理

- 我们对流式处理应用进行了改进 ， 使用新版本应用处理同一个事件流，生成新的结果，井比较两种版本的结果，然后在某个时间点将客户端切换到新的结果流上。
-  现有的流式处理应用出现了缺陷，修复缺陆之后，重新处理事件流并重新计算结果。

多版本而不是重置应用的状态会好些：

- 将新版本的应用作为一个新的消费者群组
- 让它从输入主题． 的第 一 个偏移量开始读取数据（这样它就拥有了属于自己的输入流事件副本）；
- 检查结果流，在新版本的处理作业赶上进度时，将客户端应用程序切换到新的结果流上。

第一种方案更加安全，多个版本可以来回切换，可以比较不同版本的结果，而且不会造成数据的丢失，也不会在清理过程中引入错误。

---


哪怕是一个很简单的应用，都需要一个拓扑。拓扑是由处理器组成的，这些处理器是拓扑图里的节点（用椭圆表示）。大部分处理器都实现了一个数据操作一 过滤、映射、聚合等。数据源处理器从主题上读取数据，井传给其他组件 而数据地处理器从上一个处理器接收数据，并将它们生成到主题上。拓扑总是从一个或多个数据源处理器开始，井以或多个数据地处理器结束。
