# 认识长轮询：配置中心是如何实现推送的？

> - [认识长轮询：配置中心是如何实现推送的？](https://mp.weixin.qq.com/s/YjvL0sUTGHxR3GJFqrP8qg)
> - [认识长轮询：配置中心是如何实现推送的？](https://zhuanlan.zhihu.com/p/351196920)

事实上，目前比较流行的两款配置中心：**Nacos 和 Apollo 恰恰都没有使用长连接，而是使用的长轮询**。

## 数据交互模式

众所周知，数据交互有两种模式：Push（推模式）和 Pull（拉模式）。

推模式指的是客户端与服务端建立好网络长连接，服务方有相关数据，直接通过长连接通道推送到客户端。其优点是及时，一旦有数据变更，客户端立马能感知到；另外对客户端来说逻辑简单，不需要关心有无数据这些逻辑处理。缺点是不知道客户端的数据消费能力，可能导致数据积压在客户端，来不及处理。

拉模式指的是客户端主动向服务端发出请求，拉取相关数据。其优点是此过程由客户端发起请求，故不存在推模式中数据积压的问题。缺点是可能不够及时，对客户端来说需要考虑数据拉取相关逻辑，何时去拉，拉的频率怎么控制等等。

## 长轮询与轮询

在开头，重点介绍一下长轮询（Long Polling）和轮询（Polling）的区别，两者都是拉模式的实现。

“轮询”是指不管服务端数据有无更新，客户端每隔定长时间请求拉取一次数据，可能有更新数据返回，也可能什么都没有。配置中心如果使用「轮询」实现动态推送，会有以下问题：

- 推送延迟。客户端每隔 5s 拉取一次配置，若配置变更发生在第 6s，则配置推送的延迟会达到 4s。

- 服务端压力。配置一般不会发生变化，频繁的轮询会给服务端造成很大的压力。

- 推送延迟和服务端压力无法中和。降低轮询的间隔，延迟降低，压力增加；增加轮询的间隔，压力降低，延迟增高。

“长轮询”则不存在上述的问题。客户端发起长轮询，如果服务端的数据没有发生变更，会 hold 住请求，直到服务端的数据发生变化，或者等待一定时间超时才会返回。返回后，客户端又会立即再次发起下一次长轮询。配置中心使用「长轮询」如何解决「轮询」遇到的问题也就显而易见了：

- 推送延迟。服务端数据发生变更后，长轮询结束，立刻返回响应给客户端。

- 服务端压力。长轮询的间隔期一般很长，例如 30s、60s，并且服务端 hold 住连接不会消耗太多服务端资源。

可能有人会有疑问，为什么一次长轮询需要等待一定时间超时，超时后又发起长轮询，为什么不让服务端一直 hold 住？主要有两个层面的考虑，一是连接稳定性的考虑，长轮询在传输层本质上还是走的 TCP 协议，如果服务端假死、fullgc 等异常问题，或者是重启等常规操作，**长轮询没有应用层的心跳机制，仅仅依靠 TCP 层的心跳保活很难确保可用性**，所以一次长轮询**设置一定的超时时间**也是在确保可用性。

除此之外，在配置中心场景，还有一定的业务需求需要这么设计。在配置中心的使用过程中，用户可能**随时新增配置监听**，而在此之前，长轮询可能已经发出，新增的配置监听无法包含在旧的长轮询中，所以在配置中心的设计中，一般会在一次长轮询结束后，将新增的配置监听给捎带上，而如果长轮询没有超时时间，只要配置一直不发生变化，响应就无法返回，新增的配置也就没法设置监听了。

> 见[长连接的缺点](#长连接的缺点)

## 配置中心长轮询设计

配置中心往往是为分布式的集群提供服务的，而每个机器上部署的应用，又会有多个 dataId 需要监听，实例级别 * 配置数是一个不小的数字，配置中心服务端维护这些 dataId 的长轮询连接显然不能用线程一一对应，否则会导致服务端线程数爆炸式增长。一个 Tomcat 也就 200 个线程，长轮询也不应该阻塞 Tomcat 的业务线程，所以需要**配置中心在实现长轮询时，往往采用异步响应**的方式来实现。而比较方便实现异步 HTTP 的常见手段便是 Servlet3.0 提供的 AsyncContext 机制。

## 实现细节思考

### 为什么需要定时器返回 304

上述的实现中，服务端采用了一个定时器，在配置未发生变更时，定时返回 304，客户端接收到 304 之后，重新发起长轮询。在前文，已经解释过了为什么需要超时后重新发起长轮询，而不是由服务端一直 hold，直到配置变更再返回，但可能有读者还会有疑问，为什么不由客户端控制超时，服务端去除掉定时器，这样客户端超时后重新发起下一次长轮询，这样的设计不是更简单吗？无论是 Nacos 还是 Apollo 都有这样的定时器，而不是靠客户端控制超时，这样做主要有两点考虑：

和真正的客户端超时区分开。

仅仅使用异常（Exception）来表达异常流，而不应该用异常来表达正常的业务流。304 不是超时异常，而是长轮询中配置未变更的一种正常流程，不应该使用超时异常来表达。

客户端超时需要单独配置，且需要比服务端长轮询的超时要长。正如上述的 demo 中客户端超时设置的是 40s，服务端判断一次长轮询超时是 30s。这两个值在 Nacos 中默认是 30s 和 29.5s，在 Apollo 中默认是是 90s 和 60s。

### 长轮询包含多组 dataId

在上述的 demo 中，一个 dataId 会发起一次长轮询，在实际配置中心的设计中肯定不能这样设计，一般的优化方式是，一批 dataId 组成一个组批量包含在一个长轮询任务中。在 Nacos 中，按照 3000 个 dataId 为一组包装成一个长轮询任务。

## 长轮询和长连接

讲完实现细节，本文最核心的部分已经介绍完了。再回到最前面提到的数据交互模式上提到的推模型和拉模型，其实在写这篇文章时，我曾经问过交流群中的小伙伴们“配置中心实现动态推送的原理”，他们中绝大多数人认为是长连接的推模型。然而事实上，主流的配置中心几乎都是使用了本文介绍的长轮询方案，这又是为什么呢？

我也翻阅了不少博客，显然他们给出的理由并不能说服我，我尝试着从自己的角度分析了一下这个既定的事实：

长轮询实现起来比较容易，**完全依赖于 HTTP 便可以实现全部逻辑**，而 HTTP 是最能够被大众接受的通信方式。

长轮询使用 HTTP，便于多语言客户端的编写，大多数语言都有 HTTP 的客户端。

那么长连接是不是真的就不适合用于配置中心场景呢？有人可能会**认为维护一条长连接会消耗大量资源，而长轮询可以提升系统的吞吐量**，而在配置中心场景，这一假设并没有实际的压测数据能够论证，benchmark everything！please~

另外，翻阅了一下 Nacos 2.0 的 milestone，我发现了一个有意思的规划，Nacos 的注册中心（目前是短轮询 + udp 推送）和配置中心（目前是长轮询）都有计划改造为长连接模式。

再回过头来看，长轮询实现已经将配置中心这个组件支撑的足够好了，替换成长连接，一定需要找到合适的理由才行。

## 总结

本文介绍了长轮询、轮询、长连接这几种数据交互模型的差异性。

分析了 Nacos 和 Apollo 等主流配置中心均是通过长轮询的方式实现配置的实时推送的。实时感知建立在客户端拉的基础上，因为本质上还是通过 HTTP 进行的数据交互，之所以有“推”的感觉，是因为服务端 hold 住了客户端的响应体，并且在配置变更后主动写入了返回 response 对象再进行返回。

- [简单的 demo](https://github.com/lexburner/longPolling-demo)

> 长连接优势在业务模型简单、资源利用率高、更安全，到配置场景，业务接入简单、业务请求量偏少、安全需求几乎没有，同时长连接有实现复杂、维护成本高、对研发和使用方要求更高的问题

> 长轮训可以的，首先配置中心的请求压力本就不大，长轮训有调和压力和时延，另外，基于http的长轮训对于sdk开发者也相当的简单，也很好调试。长轮询开发难度低。

---
# apollo: 长连接和长轮询
> - [apollo: 长连接和长轮询疑问](https://github.com/ctripcorp/apollo/issues/1686)

> 文档上说是：客户端和服务端保持了一个长连接，从而能第一时间获得配置更新的推送

Q: 我看代码好像startLongPolling方法定时发起一个http请求，这种http请求不应该是知连接的么，定时发起的方式，属于长轮询吧。对http长连接，tcp长链接不是很明白，能否请教一下。A: 是长轮询，实现了长连接的效果。

不算我们熟知的长连接，比如双工的 websocket 或者 tcp keep alive 。

Apollo 这里的长轮询是 client 在请求 server 的时候给了 60 秒的 timeout，然后 server 收到请求会 hang 60 秒，60秒内有相应的配置更新就立刻返回，如果没有就 60秒后返回 {}，拿到 response 的客户端再重新发起请求。


---
# Long Polling长轮询详解
> - [Long Polling长轮询详解](https://www.jianshu.com/p/d3f66b1eb748)

## 长连接的缺点
前面提到Long Polling如果当时服务端没有需要的相关数据，此时请求会hold住，直到服务端把相关数据准备好，或者等待一定时间直到此次请求超时，这里大家是否有疑问，为什么不是一直等待到服务端数据准备好再返回，这样也不需要再次发起下一次的Long Polling，节省资源？

主要原因是网络传输层主要走的是tcp协议，tcp协议是可靠面向连接的协议，通过三次握手建立连接。**但是所建立的连接是虚拟的，可能存在某段时间网络不通，或者服务端程序非正常关闭，亦或服务端机器非正常关机，面对这些情况客户端根本不知道服务端此时已经不能互通，还在傻傻的等服务端发数据过来，而这一等一般都是很长时间**。

当然tcp协议栈在实现上有**保活计时器来保证的，但是等到保活计时器发现连接已经断开需要很长时间，如果没有专门配置过相关的tcp参数，一般需要2个小时，而且这些参数是机器操作系统层面，所以，以此方式来保活不太靠谱**，故Long Polling的实现上一般是需要**设置超时时间的**。

> tcp长连接发现问题时间要比较久。保活计时器来保证的。keepalive只能检测连接是否存活，不能检测连接是否可用，而且参数是机器操作系统层面，不够灵活。

---
# 其他

- [jupiter配置中心](http://jupiter.douyu.com/juno/3.1intro.html)

---

# WEB端即时通讯：HTTP长连接、长轮询（long polling）详解
> - [WEB端即时通讯：HTTP长连接、长轮询（long polling）详解](https://www.91im.net/im/1512.html): 偏向实际应用和实战

## 相关技术优缺点对比

**轮询：**客户端定时向服务器发送Ajax请求，服务器接到请求后马上返回响应信息并关闭连接。

**优点：**后端程序编写比较容易。

**缺点：**请求中有大半是无用，浪费带宽和服务器资源。

**实例：**适于小型应用。

---

**长轮询：**客户端向服务器发送Ajax请求，服务器接到请求后hold住连接，直到有新消息才返回响应信息并关闭连接，客户端处理完响应信息后再向服务器发送新的请求。

**优点：**在无消息的情况下不会频繁的请求，耗费资源小。

**缺点：**服务器hold连接会消耗资源，返回数据顺序无保证，难于管理维护。

**实例：**WebQQ、Hi网页版、Facebook IM。

---

**长连接：**在页面里嵌入一个隐蔵iframe，将这个隐蔵iframe的src属性设为对一个长连接的请求或是采用xhr请求，服务器端就能源源不断地往客户端输入数据。

**优点：**消息即时到达，不发无用请求；管理起来也相对方便。

**缺点：**服务器维护一个长连接会增加开销。

**实例：**Gmail聊天

---

**Flash Socket：**在页面中内嵌入一个使用了Socket类的 Flash 程序JavaScript通过调用此Flash程序提供的Socket接口与服务器端的Socket接口进行通信，JavaScript在收到服务器端传送的信息后控制页面的显示。

**优点：**实现真正的即时通信，而不是伪即时。

**缺点：**客户端必须安装Flash插件；非HTTP协议，无法自动穿越防火墙。

**实例：**网络互动游戏
