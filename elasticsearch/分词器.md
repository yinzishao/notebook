## Keyword Tokenizeredit

_The keyword tokenizer is a “noop” tokenizer that accepts whatever text it is given and outputs the exact same text as a single term. It can be combined with token filters to normalise output, e.g. lower-casing email addresses._

KeywordAnalyzer把**整个输入**作为一个单独词汇单元，方便特殊类型的文本进行索引和检索。针对邮政编码，地址等文本信息使用关键词分词器进行索引项建立非常方便。

> 注意: 会区分大小写

分析器是keyword时，"QQ炫舞" 就是"QQ炫舞",**区分了大小写**, 所以`term: qq炫舞` 不会成功返回!

解决方案： 归一化处理
```
PUT index
{
  "settings": {
    "analysis": {
      "normalizer": {
        "my_normalizer": {
          "type": "custom",
          "char_filter": [],
          "filter": ["lowercase", "asciifolding"]
        }
      }
    }
  },
  "mappings": {
    "type": {
      "properties": {
        "foo": {
          "type": "keyword",
          "normalizer": "my_normalizer"
        }
      }
    }
  }
}
```
参考链接：
- <https://stackoverflow.com/a/43492634/6274400>
- <https://elasticsearch.apachecn.org/#/docs/316>
- <https://www.elastic.co/guide/en/elasticsearch/reference/5.6/normalizer.html>


---
## Standard Tokenizeredit
_The standard tokenizer provides grammar based tokenization (based on the Unicode Text Segmentation algorithm, as specified in Unicode Standard Annex #29) and works well for most languages._

英文的处理能力同于StopAnalyzer.支持中文采用的方法为单字切分。

> 他会将词汇单元转换成小写形式，并去除停用词和标点符号。

----
## Whitespace 分词器
仅仅是去除空格，对字符没有lowcase化,不支持中文； 并且不对生成的词汇单元进行其他的规范化处理。

- 参考链接： <https://zhuanlan.zhihu.com/p/29183128>

---
## IK 分词
ik_max_word: 会将文本做最细粒度的拆分，比如会将"中华人民共和国国歌"拆分为"中华人民共和国,中华人民,中华,华人,人民共和国,人民,人,民,共和国,共和,和,国国,国歌"，会穷尽各种可能的组合；

ik_smart: 会做最粗粒度的拆分，比如会将"中华人民共和国国歌"拆分为"中华人民共和国,国歌"。

----
## text的term查询
字段分析器是ik_max_word等，切词的分词时，用term达不到相应的完全匹配的效果.

某个字段的值为"QQ炫舞"， 其拆成["qq", "炫舞“], 若搜索该字段term：qq 也是成功的。term无法表达完全相等。

---
## ngram tokenizer

n-grams就像一个滑动窗口，在单词之间移动——一个指定长度的连续字符序列。它们对于查询不使用空格或具有长复合词（如德语）的语言很有用。
```
POST _analyze
{
  "tokenizer": "ngram",
  "text": "Quick Fox"
}

[ Q, Qu, u, ui, i, ic, c, ck, k, "k ", " ", " F", F, Fo, o, ox, x ]
```
参考链接:
- <https://www.elastic.co/guide/en/elasticsearch/reference/5.6/analysis-ngram-tokenizer.html#analysis-ngram-tokenizer>

---
## edge_ngram tokenizer
边缘标记器在遇到指定字符列表中的一个字符时，首先将文本分解为单词，然后它发出每个单词的n-gram，其中n-gram的开头固定在单词的开头。
```
POST my_index/_analyze
{
  "analyzer": "my_analyzer",
  "text": "2 Quick Foxes."
}

[ Qu, Qui, Quic, Quick, Fo, Fox, Foxe, Foxes ]
```
参考链接:
- <https://www.elastic.co/guide/en/elasticsearch/reference/5.6/analysis-edgengram-tokenizer.html>


---

## 繁简分词与搜索

- [繁简](./繁简.md)
