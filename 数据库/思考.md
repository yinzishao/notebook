一些共性的提炼与思考

----
# 同步ES的思路

广告出发的汇总维度表很多，当广告的一级属性、二级属性、甚至三级属性发生变化的时候，维护其聚合维度的表的成本很高。

例子： 广告根据素材进行聚合。但是广告的单一应用的应用品牌的属性（嵌套很深）发生了变化。需要同步到ES。

方案
一. 现在的方案。通过ES的update_by_query进行更新。缺点：问题版本冲突、维护链条很深、代码复杂。优点：最细粒度可控
二. 在某个地方维护一个汇总表（mysql？维护不了非关系？），从汇总表触发简单的字段同步任务。

或者一、二结合，看看什么维度能整合起来的，减少复杂性。

- 问题一： 经过一段时间的实践后，update_by_query还是有各种性能、版本冲突问题。业务上还是尽量避免。通过反查回主键id进行更新更合理。
- 问题二： 汇总冗余表同步较复杂，暂时还没考虑该方案。但是一个大宽表是很有必要的。现在的大宽表逻辑，如果是非实时的，都下沉到数据仓库里面，通过各种join进行获取。还有就是实时的通过mysql的索引更新进行同步，但是都无法很好兼容实时olap和实时更新查询的逻辑。


---
# 单表同步其他源的思路

按理说可以通过可配置化的任务来达到不写代码即可同步到es/ch的需求。

现在做不到的原因：
- 仍然未支持过滤、筛选、转换等字段函数。而大多数单表同步的任务都需要这些功能。
- 未能很好支持删除的操作。而且有一些字段为deleted的字段等逻辑也需要进行兼容。
- 单元测试？应该做到工具的覆盖各个功能的用例，而不用做到各个任务的测试？

异构数据源的检查

---

# 如何做真正的ch表

## 如何做一个准确的mysql快照

做准确一次的全量和增量还是有些难点，业界的做法是[mysql-snapshots](https://debezium.io/documentation/reference/connectors/mysql.html#mysql-snapshots), 通过锁表进行。

下面是另一个方案个人的一些思路，待生产验证：

- 消费者先在bin log的kafka占位，时间点A后面的bin log可以消费
- 通过快照读，读出一个mysql的某个时间点B的全量数据
- 消费者开启消费。时间段A到B的消息是重复数据。
- 消费完时间段A到B的消息(一段时间)后, 停止消费，需要把一些无法消除的脏数据消除掉(例如，插入了两条insert，或者插入了一条已删除的记录)
- insert into new_table select * from old_table final
    - **因为optimize tzable old_table final 无法消除这些规则不正确的脏数据。**
    - insert全量表的**性能**值得评估和思考优化增量逻辑了
- 切表，恢复消费。

做一个准确一次的表，只能通过insert into select final 进行构造了，这样的消耗了大量的资源和性能慢。

更好的方案应该是让开发理解该特性，如果是distinct 获取集合的需求，无法实时final 查询，则接受一定的时间范围延迟，无法过滤掉删除和更新的操作。通过每天定时final保证T-1的数据是正确的，而当天的最新的是能获取到的。

## 指标问题污染

而一些sum聚合函数，则要语句写得更恶心点，语句通过sign位进行消除。但前提是第一次的全量快照和后续的增量是要严格一致的，但不能出现重复插入/删除的两行。这往往很难。

问题一： 即使第一次做了一个纯净的表，但是后续的bin_log消费出错了，如果程序没有保证准确一次性消费。后面的数据也还是会有脏的。

解决方案：
- 能通过找出间隔时间段内的更改主键。然后替换或者删除相应表的数据吗？
    - 删除/替换操作其实可以当成是为了保证准确一次做不到的幂等的替换思路。复杂性较高！
    - 思路： 找出一段时间范围内的发生变更ut的主键集合。删掉正式数据库的主键值。然后插入。
        - 问题： 先删掉全量太暴力了。应该是做更新和删掉数据？但一做更新操作，也就回到覆盖更新，回到如何保证准确一次性插入的问题了。
        - 那shadow表能进行优化吗？这就需要先将主键分区，后替换掉整个分区的数据了。测试后分区后的性能差了。而且量级放大了。
            - 思路： 找出变更的主键分区，通过final插入到shadow分区，替换掉正式环境的分区。
- 保证消费端的准确一次性消费。对比一下其他开源项目的思路或者mysql的从库方案。
    - canal都无法保证准确一次生产
    - flink的exactly-once 语义。
- 查询语句写成幂等？
    - 能通过窗口函数，使得即使不是准确一次消费也能得到最终的结果吗？也就是模拟final。多线程并发条件下能写出来吗？时间成本会比final大？
- 直接final 查询？大表拉跨。

小结：还是得往如何作准确一次的思路靠？只要能真实还原从库的操作。可以通过sign等数据库操作进行数据获取。

根源思考： 榜单类的统计指标，真的需要实时吗？没有一点延迟吗？替换数据本身就会有各种原子性，延迟问题？还是得对比Flink等别人的思路。

## 索引效率

问题二： 折叠树的主键是跟mysql是一致的，主键进行分区后发现in 主键的查询性能差了很多。而且导致其他范围查询列无法过滤数据块，较慢。

解决方案：
- 通过primary key 和
- 物化视图。那如何解决删除相关的逻辑？alter delete操作?

## 参考链接
- [Clickhouse issue: Supporting exactly once](https://github.com/ClickHouse/ClickHouse/issues/7522): 仅仅是讨论了zk的插入块的去重，提供kafka引擎的思路。

## 总结

binlog实时等监听捕获，同步到目的数据源的方案。因为需要保证端到端整个链条（**canal本身不是准确一次的**，消费端也需要保证准确一次）的准确一次较难实现。如果无法保证，则难以保证数据的准确性，也无法用到生产环境上。

对于一些能满足容忍重复数据或者错误数据的业务场景，能直接用实时表。例如distinct查询。

准确的增量数据的同步大部分通过批量的数据同步等操作，覆盖掉会更改的分区， 这样是能够准确的同步数据。但一般从mysql拉取整个分区的数据速度较慢。

可以提供一个思路： 是通过binlog等实时流进目的源。虽然不是准确的数据，但也保证了最少一次。在目的源等大数据引擎上，定时通过，查询语法(final)或者引擎本身支持的覆盖更新，折叠数等特性操作，获取到增量更改分区的另一份的准确数据。以此覆盖掉落后的分区数据。


----


# 业务思考

CC数据和AG的数据共性

## 概念

基本属性表+组合（一对一、一对多、多对多）
    - 广告属性表、商品属性表、素材属性表、品牌属性表
    - 作品属性表、商品属性表、达人属性表、品牌属性表

指标值（时序数据）
    - 商品的每天新增销量
    - 广告的每天金额预估
    - 作品的每天新增点赞数

## 场景

### 多维度的uv和pv需求
根据各个基本表+组合的任意维度的基本属性筛选
    - 获取任意时间段的广告的品牌的xx行业
    - 获取达人的任意时间段的作品的商品的属性值筛选

### 多维度的任意时间段的时序数据聚合需求
根据筛选条件进行时序数据的集合汇总
    - 以上同样筛选条件的指标聚合数据。
    - 根据聚合数据进行排序或者筛选

### 多维度的属性分析：uv、pv和数序数据聚合需求
根据筛选条件按照属性字段的进行uv、pv、数序数据聚合汇总

例子: 结合商品的各种关联属性，进行筛选某一个时间范围内的商品的品类属性分析。时间段内新增销量统计（数序数据）, 去重商品数（uv）, 最低/高商品价格（min/max）。


### 文本搜索需求

根据各种文本属性进行搜索

## 需求

以上几个场景+各种表的组合


## 解决办法

### 多维度的uv和pv需求通过位图进行优化

存在的问题是
    - 维度过大导致维度列的组合放大，位图效果不佳。需要限定筛选维度列数。
    - 需要提前做好大宽表。或者做聚合表的时候join查询，则对数据库要求较高。

**但一般会加上时序数据的需求**

最直观的方案，是时序数据一般会冗余基本属性字段。**但是因为属性字段一变化，需要把全量的数据进行更改**！

例子： 某个商品的每天新增销量统计，这个是按天维度的时序数据。结合商品的各种关联属性，进行筛选。

### 大宽表方案

1. 大宽表方案：AG的ad_aggs_outer则是利用月汇总表和通过b+树索引，**来达到属性的更新和量级的降重**。然后再每天重新展开该表来符合任意时间段的筛选。

2. 数据仓库，mysql实时流进数据仓库，每天进行T-1的join表达到大宽表。

而这些方案的另一个问题则是，**有些指标列无法冗余到一对多宽表中**。

如果是一对多的属性，如果拆成多行，则会**把时序数据的列指标扩展了，与事实不符**。

- 要不就是指标是跟着一对多属性的多列分配好的，不然无法把数序数据和属性值组合冗余。
- 要不就是在查询的时候进行去重。product_sold_day(商品对品类一对多冗余): [例子，需要语句去重](https://redash-alishh.umlife.net/queries/4848/source) 、 [错误例子，没有去重](https://redash-alishh.umlife.net/queries/2429/source)

例子： 一个广告的金额，关联单一品牌，但是品牌会有多个行业属性，无法把广告的金额行拆成多行。

解决方案：

**分离基本属性的筛选和时序数据的筛选表，最后再进行组合**。

注意**排序问题**。不然的话，拿到当页的局限数据，进行单页的筛选和查询信息也很快。

但排序的话，需要进行宽表和指标表的组合。需要**sql里面join**或者**程序进行代码的连接和排序**。但都会有相应的性能问题。

问题：
    - join的性能问题
    - 如果程序进行排序的性能问题

为了避免join的解决办法：

- 先根据排序字段去到相关表进行筛选、聚合统计、排序。**返回限定页的数据，然后去另一个表进行剩余数据的补充**。业务代码较复杂。
    - 例子: 如果是根据金额排序，则去时序数据表，进行金额的筛选排序后得到相关的数据单页数据。再去维度统计表算出单页数量的数据的uv值。[时序先拿](https://redash-alishh.umlife.net/queries/7685/source)
    - 但属性值和时序统计值都需要过滤的话，仍然需要join。

- 产品设计更改，避免两个需求的组合。


### 搜索需求

解决方案： 搜索需求则要通过ES进行筛选，根据得分返回TOP N的集合，进行in查询。

问题： 会导致返回结果不完全，需要产品了解特性，如果返回集合过多其实也是因为用户筛选的关键词太广泛了，返回的过多数据其实对用户也是没有太大价值的。

---

# 近X天指标统计需求

## 需求

- 获取用户在某个媒体最后一次直播时间。last/max
- 获取近30天直播场数。UV
- 获取微博、小红书近30天文章和短视频数量。UV
- 获取用户最近带货商品数。join表查询

先从dla到mysql，而不是直接到es，es跟mysql一对一监听binlog字段映射。
- es版本冲突问题，导致数据不一致。
- yugong2mysql灵活方便，方便调整业务。
- mysql的binlog可以同步到各种下游。
- 经验来说后续业务，仍然需要join各种关联表。往es里面仍成本较高。
