# 多维分析中的 UV 与 PV

对于互联网产品来说，UV 与 PV 是两个非常常见的指标，并且通常都是分析的最基础指标。UV 一般来讲，是指使用产品(或产品某个功能)的独立用户数。PV 则来源于网站时代，一般指网站(或网站某个页面)的页面浏览量，在移动互联网时代，则一般会引申表示使用产品(或产品某个功能)的用户行为或者用户操作数量。

## “可加”与“不可加”

在具体的实现中，有一种最为常见的实现手段，就是把各个维度的所有取值组合下的指标全部预先计算并且存储好，这种一般可以称作事实表。然后在具体进行多维查询的时候，再根据维度的选择，扫描相对应的数据，并聚合得到最终的查询条件。

但是，对于 UV 这类指标，却不能简单的累加，因为，这个指标并不是在每一个维度上都是正交的。例如，同一个用户可能先后使用了不同的 App 版本，甚至于有一定几率使用了不同的终端，所以，UV 并不能简单地累加，通常情况下，真实的 UV 是比加起来的值更小的。

而对于AG来说, UV可以理解为筛选时间范围内的投放广告数，同一个广告先后投放多天。

因而，对于 UV 这类不可累加的指标，需要使用其它的计算方案。

## UV 计算的常见方案

UV 类型的指标，有三种常见的计算方案，我们在这里分别进行介绍。

### 估算方案

所谓的估算方案，就是在上面的表格的基础上，不再额外记录更多细节，而是通过估算的方式来给出一个接近真实值的 UV 结果，常见的算法有很多，例如 HyperLogLog 等。

[ClickHouse提供各种各样在允许牺牲数据精度的情况下对查询进行加速的方法](https://clickhouse.tech/docs/zh/introduction/distinctive-features/#zhi-chi-jin-si-ji-suan):

*   用于近似计算的各类聚合函数，如：distinct values, medians, quantiles
*   基于数据的部分样本进行近似查询。这时，仅会从磁盘检索少部分比例的数据。
*   不使用全部的聚合条件，通过随机选择有限个数据聚合条件进行聚合。这在数据聚合条件满足某些分布条件下，在提供相当准确的聚合结果的同时降低了计算资源的使用。

由于毕竟是估算，最终估算的结果有可能与真实值有较大差异，因此只有一些统计平台可能会采用。

### 扩充事实表，以存代算

MOLAP（Multidimensional OLAP，多维型OLAP）: 其核心思想是**借助预先聚合结果，使用空间换取时间的形式最终提升查询性能**。

所谓以存代算，就是在预先计算事实表的时候，将所有需要聚合的结果，**预先做完聚合都算好**，类似于 Hive 所提供的group by with cube操作。

缺点：

- 由于预先聚合只能支持固定的分析场景，所以它无法满足自定义分析的需求。
- 无法进行多选，除非把多选也作为维度，维度组合爆炸会导致数据膨胀，这样会造成不必要的计算和存储开销。
- 预处理的形式，数据立方体会有一定的滞后性，不能实时进行数据分析。



### 从最细粒度数据上扫描

ROLAP(Relational Online Analytical Processing): 之前提出的**扩充事实表**的方式，的确可以解决多维分析中指标聚合的问题，除此之外，还有一种方案，则是在事实表上，将用户ID也做为一个维度，来进行保存，此时就不需要保存 UV 了

对于用户的每个行为，都保留一条数据。

![](.多维分析中的UV与PV_images/d7c0b838.png)


虽然这样一来，需要保存的数据规模有了**数量级上的扩充**，并且所有的聚合计算都需要在多维分析查询的时候再扫描数据并进行聚合，存储和计算代价都提高了很多，看似是一种很无所谓的举措。

但是，相比较之前的方案，它却有一个最大的好处，也即是因为有了**最细粒度的用户行为数据**，才有可能计算事件级别的漏斗、留存、回访等，才有可能在这些数据的基础之上，进一步做用户画像、个性化推荐等等。

- 缺点：数量级上的扩充，实时扫描聚合代价更高。
- 优点：最细粒度的用户行为数据。


在这样一个数据存储方案的基础上，为了提高数据查询的效能，一般的优化思路有:

- **采用列存储加压缩、位图等方式减少从磁盘中扫描的数据量**，
- **采用分布式的方案提高并发扫描的性能**，
- **采用应用层缓存来减少不同查询的公共扫描数据的量等等**。


### 位图压缩方案使用场景

位图的使用约束: 不适用于对于维度去重率不高的表设计，[ad_aggs_by_day_style基数分析](https://redash-alishh.umlife.net/queries/10891#14375)。

对于uniqExact(style_id, campaign_id, area, dt)去重的基数来说，仅仅是当天总行数的85%。对于将这些维度爆炸来进行预聚合操作没有太大的意义，原因如下：

- 第一位图的运算在行数过大的情况下，比单纯的聚合会更慢。
- 第二位图的空间成本其实也是更大的，行数过大甚至会导致会比旧的数据更大。

可以从业务出发对表结构做相关的优化思路。例如如果是时间范围的总聚合指标，可以通过按周/按月进行相关的汇总表进行降维的操作，减少扫描量，但是需要在业务层把各个时间进行组合查询，造成业务sql复杂化。

# 参考链接：

- [Difference between ROLAP, MOLAP, and HOLAP](https://www.javatpoint.com/rolap-vs-molap-vs-holap)
- [多维分析中的 UV 与 PV ](https://www.sohu.com/a/115979730_116235)
- [Flink 在快手实时多维分析场景的应用](https://www.infoq.cn/article/zkz1vpe3qgyfrutb6pcm)

