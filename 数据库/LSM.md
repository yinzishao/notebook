# LSM Tree存储结构

> - [LSM Tree存储组织结构介绍](https://www.cnblogs.com/bangerlee/p/4307055.html)

LSM tree存储实现思路与以上四种措施不太相同，其将随机写转化为顺序写，尽量保持日志型数据库的写性能优势，并提供相对较好的读性能。具体实现方式如下：

1. 当有写操作(或update操作)时，**写入位于内存的buffer**，内存中通过某种数据结构(如skiplist)保持key有序
2. 一般的实现也会将数据**追加写到磁盘Log文件**，以备必要时恢复
3. 内存中的数据**定时或按固定大小地刷到磁盘**，更新操作只不断地写到内存，并不更新磁盘上已有文件
4. 随着越来越多写操作，**磁盘上积累的文件**也越来越多，这些文件不可写且有序
5. 定时对文件进行**合并操作**(compaction)，消除冗余数据，减少文件数量

> clickhouse的合并树、Elasticsearch的合并段


---
# 时序数据库
> - [十分钟看懂时序数据库 - 存储](https://juejin.cn/post/6844903477856960526)

如果只是存储起来，直接写成日志就行。但因为后续还要快速的查询，所以需要考虑存储的结构。

传统数据库存储采用的都是B tree，这是由于其在查询和顺序插入时**有利于减少寻道次数**的组织形式。我们知道磁盘寻道时间是非常慢的，一般在10ms左右。磁盘的随机读写慢就慢在寻道上面。对于随机写入B tree会**消耗大量的时间在磁盘寻道上，导致速度很慢**。我们知道SSD具有更快的寻道时间，但并没有从根本上解决这个问题。

对于90%以上场景都是写入的时序数据库，B tree很明显是不合适的。业界主流都是采用LSM tree替换B tree，比如Hbase, Cassandra等nosql中。这里我们详细介绍一下。

LSM tree包括**内存里的数据结构和磁盘上的文件**两部分。分别对应Hbase里的MemStore和HLog;对应Cassandra里的MemTable和sstable。

LSM tree操作流程如下：

1. 数据写入和更新时首**先写入位于内存里的数据结构**。为了避免数据丢失也会先写到**WAL文件**中。
2. 内存里的数据结构会**定时或者达到固定大小会刷到磁盘**。这些磁盘上的文件**不会被修改**。
3. 随着磁盘上积累的文件越来越多，会**定时的进行合并操作**，消除冗余数据，减少文件数量。

可以看到LSM tree核心思想就是**通过内存写和后续磁盘的顺序写入获得更高的写入性能，避免了随机写入**。但同时也牺牲了读取性能，因为同一个key的值可能存在于多个HFile中。为了获取更好的**读取性能**，可以通过bloom filter和compaction得到，这里限于篇幅就不详细展开。

可以看到各分布式时序数据库虽然存储方案都略有不同，但本质上是一致的，由于时序数据**写多读少**的场景，在单机上采用更加适合大吞吐量写入的单机存储结构，而在分布式方案上根据时序数据的特点来精心设计，目标就是设计的**分片方案**能方便时序数据的写入和读取，同时使数据分布更加均匀，尽量避免热点的产生。

数据存储是时序数据库设计中很小的一块内容，但也能管中窥豹，看到时序数据库从设计之初就要考虑时序数据的特点。后续我们会从其他的角度进行讨论。
