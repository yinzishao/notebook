# 实现

> - [分布式锁的实现之 redis 篇](https://xiaomi-info.github.io/2019/12/17/redis-distributed-lock/)

Redis 锁主要利用 Redis 的 setnx 命令。

- 加锁命令：SETNX key value，**当键不存在时**，对键进行设置操作并返回成功，**否则返回失败**。KEY 是锁的唯一标识，一般按业务来决定命名。
- 解锁命令：DEL key，通过删除键值对释放锁，以便其他线程可以通过 SETNX 命令来获取锁。
- 锁超时：EXPIRE key timeout, 设置 key 的超时时间，以保证即使锁没有被显式释放，锁也可以在一定时间后自动释放，避免资源被永远锁住。

SETNX: SET if Not exists, SetNX 不具备设置过期时间的功能

## 1. SETNX 和 EXPIRE 非原子性

如果 SETNX 成功，在**设置锁超时时间后**，服务器挂掉、重启或网络问题等，导致 EXPIRE 命令没有执行，**锁没有设置超时时间变成死锁**。

解决的办法：使用set的命令，同时设置锁和过期时间。Redis2.6.12以上版本，可以用set获取锁。set可以实现setnx和expire，这个是原子操作。

```
set key value [EX seconds] [PX milliseconds] [NX|XX]

EX seconds：设置失效时长，单位秒
PX milliseconds：设置失效时长，单位毫秒
NX：key不存在时设置value，成功返回OK，失败返回(nil)
XX：key存在时设置value，成功返回OK，失败返回(nil)
```

## 2. 锁误解除

如果线程 A 成功获取到了锁，并且设置了过期时间 30 秒，但线程 A 执行时间超过了 30 秒，锁过期自动释放，此时线程 B 获取到了锁；随后 A 执行完成，线程 A 使用 DEL 命令来释放锁，但此时线程 B 加的锁还没有执行完成，线程 A 实际释放的线程 B 加的锁。

通过在 value 中设置当前线程加锁的标识，在删除之前验证 key 对应的 value 判断锁是否是当前线程持有。可生成一个 UUID 标识当前线程，**使用 lua 脚本做验证标识和解锁操作**。

```bash
// 加锁
String uuid = UUID.randomUUID().toString().replaceAll("-","");
SET key uuid NX EX 30
// 解锁
if (redis.call('get', KEYS[1]) == ARGV[1])
    then return redis.call('del', KEYS[1])
else return 0
end
```

## 3. 超时解锁导致并发

如果线程 A 成功获取锁并设置过期时间 30 秒，但**线程 A 执行时间超过了 30 秒**，锁过期自动释放，此时线程 B 获取到了锁，线程 A 和线程 B 并发执行。


A、B 两个线程发生并发显然是不被允许的，一般有两种方式解决该问题：

- 将过期时间设置足够长，确保代码逻辑在锁释放之前能够执行完成。
- 为获取锁的线程**增加守护线程**，为将要过期但未释放的锁**增加有效时间**。

> 见下： redisson在加锁成功后，会注册一个定时任务监听这个锁，每隔10秒就去查看这个锁，如果还持有锁，就对过期时间进行续期。默认过期时间30秒。这个机制也被叫做：“看门狗”。如果中间宕机，锁超过超时，守护线程也不在了，自动释放锁。

## 4. 不可重入

当线程**在持有锁的情况下再次请求加锁**，如果一个锁支持一个线程多次加锁，那么这个锁就是可重入的。如果一个不可重入锁被再次加锁，由于该锁已经被持有，再次加锁会失败。Redis 可通过对锁进行重入计数，加锁时加 1，解锁时减 1，当计数归 0 时释放锁。

在**本地记录记录重入次数**，如 Java 中使用 ThreadLocal 进行重入次数统计，简单示例代码：

本地记录重入次数虽然高效，但如果考虑到**过期时间和本地、Redis 一致性的问题，就会增加代码的复杂性**。另一种方式是 **Redis Map 数据结构来实现分布式锁**，既存锁的标识也对重入次数进行计数。

## 5. 无法等待锁释放

上述命令执行都是**立即返回的**，如果客户端可以**等待锁释放**就无法使用。

- 可以通过客户端**轮询**的方式解决该问题，当未获取到锁时，等待一段时间重新获取锁，直到成功获取锁或等待超时。这种方式比较消耗服务器资源，当并发量比较大时，会影响服务器的效率。
- 另一种方式是使用 Redis 的**发布订阅**功能，当获取锁失败时，**订阅锁释放消息**，获取锁成功后释放时，发送锁释放消息。

# 三、集群
## 1. 主备切换
为了保证 Redis 的可用性，一般采用主从方式部署。主从数据同步有异步和同步两种方式，Redis 将指令记录在本地内存 buffer 中，然后异步将 buffer 中的指令同步到从节点，从节点一边执行同步的指令流来达到和主节点一致的状态，一边向主节点反馈同步情况。

在包含主从模式的集群部署方式中，**当主节点挂掉时，从节点会取而代之**，但客户端无明显感知。当客户端 A 成功加锁，指令还未同步，此时主节点挂掉，从节点提升为主节点，新的主节点没有锁的数据，当客户端 B 加锁时就会成功。

## 2. 集群脑裂

集群脑裂指因为网络问题，导致 Redis master 节点跟 slave 节点和 sentinel 集群处于不同的网络分区，因为 sentinel 集群无法感知到 master 的存在，所以将 slave 节点提升为 master 节点，此时存在两个不同的 master 节点。Redis Cluster 集群部署方式同理。

当不同的客户端连接不同的 master 节点时，两个客户端可以同时拥有同一把锁。

---
# Redis分布式锁
> - [redis分布式锁的这些坑，我怀疑你是假的开发](https://my.oschina.net/u/4526289/blog/4946239)


### 续期

redis分布式锁过期，而业务逻辑没执行完的场景。为了解决这个问题我们使用redis客户端redisson，redisson很好的解决了redis在分布式环境下的一些棘手问题，它的宗旨就是让使用者减少对Redis的关注，将更多精力用在处理业务逻辑上。

redisson对分布式锁做了很好封装，只需调用API即可。

`RLock lock = redissonClient.getLock("stockLock");`

redisson在加锁成功后，会注册一个定时任务监听这个锁，每隔10秒就去查看这个锁，如果还持有锁，就对过期时间进行续期。默认过期时间30秒。这个机制也被叫做：“看门狗”


### 主从复制的坑

redis cluster集群环境下，假如现在A客户端想要加锁，它会根据路由规则选择一台master节点写入key mylock，在加锁成功后，master节点会把key异步复制给对应的slave节点。

如果此时redis master节点宕机从节点复制失败，为保证集群可用性，会进行主备切换，slave变为了redis master。B客户端在新的master节点上加锁成功，而A客户端也以为自己还是成功加了锁的。另外如果主从复制延迟同样也会造成加锁和解锁延迟的问题。

此时就会导致同一时间内多个客户端对一个分布式锁完成了加锁，导致各种脏数据的产生。

**毕竟redis是保持的AP而非CP，如果要追求强一致性可以使用zookeeper分布式锁。**

---
# Redlock算法
> - [基于Redis实现分布式锁之前，这些坑你一定得知道](https://juejin.cn/post/6844904086236561416)


对于第一个单点问题，顺着redis的思路，接下来想到的肯定是Redlock了。Redlock为了解决单机的问题，需要多个（大于2）redis的master节点，多个master节点互相独立，没有数据同步。
Redlock的实现如下：

1. 获取当前时间。
2. 依次获取N个节点的锁。 每个节点加锁的实现方式同上。这里有个细节，就是每次获取锁的时候的过期时间都不同，需要减去之前获取锁的操作的耗时。

    - 比如传入的锁的过期时间为500ms，
    - 获取第一个节点的锁花了1ms，那么第一个节点的锁的过期时间就是499ms，
    - 获取第二个节点的锁花了2ms，那么第二个节点的锁的过期时间就是497ms
    - 如果锁的过期时间小于等于0了，说明整个获取锁的操作超时了，整个操作失败

3. 判断是否获取锁成功。 如果client在上述步骤中获取到了(N/2 + 1)个节点锁，并且每个锁的过期时间都是大于0的，则获取锁成功，否则失败。失败时释放锁。
4. 释放锁。 对所有节点发送释放锁的指令，每个节点的实现逻辑和上面的简单实现一样。为什么要对所有节点操作？因为分布式场景下从一个节点获取锁失败不代表在那个节点上加速失败，可能实际上加锁已经成功了，但是返回时因为网络抖动超时了。

以上就是大家常见的redlock实现的描述了，一眼看上去就是简单版本的多master版本，如果真是这样就太简单了，接下来分析下这个算法在各个场景下是怎样被玩坏的。

- 性能问题。

    获取锁的时间上。如果redlock运用在高并发的场景下，存在N个master节点，一个一个去请求，耗时会比较长，从而影响性能。这个好解决。通过上面描述不难发现，从多个节点获取锁的操作并不是一个同步操作，可以是异步操作，这样可以多个节点同时获取。即使是并行处理的，还是得预估好获取锁的时间，保证锁的TTL > 获取锁的时间+任务处理时间。

    被加锁的资源太大。加锁的方案本身就是会为了正确性而牺牲并发的，牺牲和资源大小成正比。这个时候可以考虑对资源做拆分，拆分的方式有两种：

    - 从业务上将锁住的资源拆分成多段，每段分开加锁。

    - 用分桶的思想，将一个资源拆分成多个桶，一个加锁失败立即尝试下一个。

- 重试问题。

    无论是简单实现还是redlock实现，都会有重试的逻辑。如果直接按上面的算法实现，是会存在多个client几乎在同一时刻获取同一个锁，然后每个client都锁住了部分节点，但是没有一个client获取大多数节点的情况。解决的方案也很常见，在重试的时候让多个节点错开，错开的方式就是在重试时间中加一个随机时间。这样并不能根治这个问题，但是可以有效缓解问题，亲试有效。


- 节点宕机。没有做持久化的场景

    最容易想到的方案是打开持久化。持久化可以做到持久化每一条redis命令，但这对性能影响会很大，一般不会采用，如果不采用这种方式，在节点挂的时候肯定会损失小部分的数据，可能我们的锁就在其中。

    另一个方案是延迟启动。就是一个节点挂了修复后，不立即加入，而是等待一段时间再加入，等待时间要大于宕机那一刻所有锁的最大TTL。

    但这个方案依然不能解决问题，如果在上述步骤3中B和C都挂了呢，那么只剩A、D、E三个节点，从D和E获取锁成功就可以了，还是会出问题。那么只能增加master节点的总量，缓解这个问题了。增加master节点会提高稳定性，但是也增加了成本，需要在两者之间权衡。


- 没发保证client的执行时间一定小于锁的TTL。一种解决方案是不设置TTL，而是在获取锁成功后，给锁加一个watchdog，watchdog会起一个定时任务，在锁没有被释放且快要过期的时候会续期。

    不过这种做法也无法百分百做到同一时刻只有一个client获取到锁，**如果续期失败**，比如发生了Martin Kleppmann所说的STW的GC，或者client和redis集群失联了，只要续期失败，就会造成同一时刻有多个client获得锁了。

    这里也提下Martin Kleppmann的解决方案，我自己觉得这个方案并不靠谱，原因后面会提到。 他的方案是让加锁的资源自己维护一套**保证不会因加锁失败而导致多个client在同一时刻访问同一个资源的情况**。

- 系统时钟漂移。

    为什么系统时钟会存在漂移呢？先简单说下系统时间，linux提供了两个系统时间：clock realtime和clock monotonic。clock realtime也就是xtime/wall time，这个时间时可以被用户改变的，被NTP改变，gettimeofday拿的就是这个时间，redis的过期计算用的也是这个时间。
    clock monotonic ，直译过来时单调时间，不会被用户改变，但是会被NTP改变。

    最理想的情况时，所有系统的时钟都时时刻刻和NTP服务器保持同步，但这显然时不可能的。导致系统时钟漂移的原因有两个：

    - 系统的时钟和NTP服务器不同步。这个目前没有特别好的解决方案，只能相信运维同学了。
    - clock realtime被人为修改。在实现分布式锁时，不要使用clock realtime。不过很可惜，redis使用的就是这个时间，我看了下Redis 5.0源码，使用的还是clock realtime。Antirez说过改成clock monotonic的，不过大佬还没有改。也就是说，人为修改redis服务器的时间，就能让redis出问题了。

> 需要注意高并发的加锁和解锁不是同一个线程。所以解锁的时候要判断是否是加的锁。[redis分布式锁](https://www.bilibili.com/video/BV1Hy4y1B78T?p=60)

> 坑爹的redis分布式锁，太多问题和场景了。没有都深入了解


---
# 实例一则

在一次线上支付后，流程引擎收到了两次统一支付的通知

查看log发现同一个订单触发了两次订单状态转换，并且两次转换的时间非常接近

希望实现当一个进程（或线程）在读取订单状态并执行订单状态转换的时候，其他进程（或线程）阻塞等待。数据库行锁和redis锁都可以实现这个需求

## select ... for update

### 用法
select * from goods where id = 1 for update;

### 申请前提
如果一条数据已经被select_for_update锁住，那么另一个进程对它的select_for_update申请就会阻塞

### 使用条件
for update仅适用于InnoDB，且必须在事务块(BEGIN/COMMIT)中才能生效。

### 使用效果
在进行事务操作时，通过“for update”语句，MySQL会对查询结果集中每行数据都添加排他锁，其他进程对该记录的更新与删除操作都会阻塞，但读数据的操作不会阻塞。

### 行锁与表锁
InnoDB行锁是通过给索引上的索引项加锁来实现的，只有通过索引条件检索数据，InnoDB才使用行级锁，否则，InnoDB将使用表锁。

### 不足

如果调用流程引擎之后、事务之内代码出错，那么这个事务就不会提交，订单状态不会被修改，但是，process_engine_callback会被执行

这就导致订单状态仍是未支付，但是已经通知了流程引擎，下一次支付成功的回调到来时，又会触发一次订单状态转换、调用流程引擎

> 分布式事务场景。sql回滚，但是外部事务没有进行回滚。

## redis

### 用法

略

### 使用自旋锁

当用户刷新订单详情页面，会向第三方渠道请求订单状态，如果第三方渠道回复订单状态是已支付，**这时也会发生订单状态转换，所以在这个地方也有锁**

如果一旦setnx失败就返回报错，用户体验就不好。因此，**最好阻塞进程直到获取到锁**。

### 解决select_for_update方法的不足

再来看这段代码，改用redis锁后，如果在调用流程引擎之后出错，订单的状态也已经在order_info.save()的时候改变了，所以，下一次支付成功的回调到来，不会再次触发订单状态转换

> 错误理解： 跟select_for_update方法加个try catch final 提交事务，没有区别。这里解决的问题应该是通过分布式锁来解决并发(幂等)问题。要注意分离幂等性问题、分布式锁、分布式事务的问题。

> 不足和核心原因分析思路有误: 不能保证流程引擎的分布式事务问题，例如失败回滚。实现真正的分布式事务，思路可以是通过异步队列进行通知。而异步队列实现精确（涉及到另一个知识点: 消息队列的精准一次）/至少一次(幂等)的操作。保证最终一致性。

## 非分布式锁的解决办法

通过mysql的状态更新进行解决。也可以说让**并发进去的函数保持了幂等性**。而不是通过复杂、很重的锁来限制并发。

`update set status = 1 where status = 0;` 通过判断旧状态到新状态的改变的行数，以此来进行并发的锁。如果改变行，则执行下面的流程，否则证明有另一个并发的程序已经执行了，直接退出。

在上面的例子中的场景就是：添加一个中间状态，支付中，如果支付成功，需要通过数据库进行默认where status = 支付中的条件进行更新，如果更新不改变行数则直接跳过，以此达到分布式锁。

而至于说整体流程的事务原子性、一致性等，那就是分布式事务的主题了。需要考虑幂等重试和定时补偿了。
